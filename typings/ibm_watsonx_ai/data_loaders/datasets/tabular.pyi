"""
This type stub file was generated by pyright.
"""

import pandas as pd
from pathlib import Path
from functools import cached_property
from typing import Any, Iterator, TYPE_CHECKING
from ibm_watsonx_ai.helpers.connections.local import LocalBatchReader
from ibm_watsonx_ai.helpers.connections import DataConnection
from ibm_watsonx_ai.helpers.connections.flight_service import FlightConnection
from torch.utils.data import IterableDataset

__all__ = ["TabularIterableDataset"]
if TYPE_CHECKING:
    ...
DEFAULT_SAMPLE_SIZE_LIMIT = ...
DEFAULT_SAMPLING_TYPE = ...
DEFAULT_DOCUMENTS_SAMPLING_TYPE = ...
logger = ...
class TabularIterableDataset(IterableDataset):
    """
    Iterable class downloading data in batches.

    :param connection: connection to the dataset
    :type connection: DataConnection

    :param experiment_metadata: metadata retrieved from the experiment that created the model
    :type experiment_metadata: dict, optional

    :param enable_sampling: if set to `True`, will enable sampling, default: True
    :type enable_sampling: bool, optional

    :param sample_size_limit: upper limit for the overall data to be downloaded in bytes, default: 1 GB
    :type sample_size_limit: int, optional

    :param sampling_type: a sampling strategy on how to read the data,
        check `SamplingTypes` enum class for more options
    :type sampling_type: str, optional

    :param binary_data: if set to `True`, the downloaded data will be treated as binary data
    :type binary_data: bool, optional

    :param number_of_batch_rows: number of rows to read in each batch when reading from the flight connection
    :type number_of_batch_rows: int, optional

    :param stop_after_first_batch: if set to `True`, the loading will stop after downloading the first batch
    :type stop_after_first_batch: bool, optional

    :param total_size_limit: upper limit for overall data to be downloaded in Bytes, default: 1 GB,
        if more than one of: `total_size_limit`, `total_nrows_limit`, `total_percentage_limit` are set,
        then data are limited to the lower threshold, if None, then all data are downloaded in batches
        in the `iterable_read` method
    :type total_size_limit: int, optional

    :param total_nrows_limit: upper limit for overall data to be downloaded in a number of rows,
        if more than one of: `total_size_limit`, `total_nrows_limit`, `total_percentage_limit` are set,
        then data are limited to the lower threshold
    :type total_nrows_limit: int, optional

    :param total_percentage_limit: upper limit for overall data to be downloaded in percent of all dataset,
        must be a float number between 0 and 1, if more than one of: `total_size_limit`, `total_nrows_limit`,
        `total_percentage_limit` are set, then data are limited to the lower threshold
    :type total_percentage_limit: float, optional

    :param apply_literal_eval: when True then ast.literal_eval will be applied to all string columns.
    :type apply_literal_eval: bool, optional

    :param cast_strings: when True then all string columns are cast to float or bool if applicable
    :type cast_strings: bool, optional

    **Example:**

        .. code-block:: python

            experiment_metadata = {
                "prediction_column": "species",
                "prediction_type": "classification",
                "project_id": os.environ.get("PROJECT_ID"),
                "credentials": credentials,
            }

            connection = DataConnection(
                data_asset_id="5d99c11a-2060-4ef6-83d5-dc593c6455e2"
            )


    **Example: default sampling - read first 1 GB of data**

        .. code-block:: python

            iterable_dataset = TabularIterableDataset(connection=connection,
                                                      enable_sampling=True,
                                                      sampling_type='first_n_records',
                                                      sample_size_limit = 1GB,
                                                      experiment_metadata=experiment_metadata)

    **Example: read all data records in batches/no subsampling**

        .. code-block:: python

            iterable_dataset = TabularIterableDataset(
                connection=connection,
                enable_sampling=False,
                experiment_metadata=experiment_metadata,
            )

    **Example: stratified/random sampling**

        .. code-block:: python

            iterable_dataset = TabularIterableDataset(connection=connection,
                                                      enable_sampling=True,
                                                      sampling_type='stratified',
                                                      sample_size_limit = 1GB,
                                                      experiment_metadata=experiment_metadata)

    """
    def __init__(self, connection: DataConnection | dict, experiment_metadata: dict | None = ..., enable_sampling: bool = ..., sample_size_limit: int = ..., sampling_type: str = ..., binary_data: bool = ..., number_of_batch_rows: int | None = ..., stop_after_first_batch: bool = ..., total_size_limit: int = ..., total_nrows_limit: int | None = ..., total_percentage_limit: float = ..., apply_literal_eval: bool = ..., cast_strings: bool = ..., **kwargs: Any) -> None:
        ...
    
    @cached_property
    def connection(self) -> FlightConnection | LocalBatchReader:
        """
        Get data connection.

        :returns: connection used in data operations
        :rtype: FlightConnection | LocalBatchReader

        **Example:**

        .. code-block:: python

            dataset = TabularIterableDataset(...)
            conn = dataset.connection

            # Your code here...

            conn.close()  # FlightConnection instances must be closed after use
        """
        ...
    
    def write(self, data: pd.DataFrame | None = ..., file_path: str | Path | None = ...) -> None:
        """
        Writes data into the data source connection.

        :param data: structured data to be saved in data source connection, 'data' or 'file_path' must be provided
        :type data: DataFrame, optional

        :param file_path: path to the local file to be saved in a source data connection (binary transfer).
            'data' or 'file_path' need to be provided
        :type file_path: str | Path, optional
        """
        ...
    
    def __iter__(self) -> Iterator:
        """Iterate over Flight Dataset."""
        ...
    


