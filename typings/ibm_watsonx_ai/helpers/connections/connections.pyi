"""
This type stub file was generated by pyright.
"""

from pathlib import Path
from typing import Any, List, Optional, TYPE_CHECKING, Tuple, Union
from .base_connection import BaseConnection
from .base_data_connection import BaseDataConnection
from .base_location import BaseLocation
from pandas import DataFrame

__all__ = ["DataConnection", "S3Connection", "ConnectionAsset", "S3Location", "FSLocation", "AssetLocation", "CloudAssetLocation", "DeploymentOutputAssetLocation", "NFSConnection", "NFSLocation", "ConnectionAssetLocation", "DatabaseLocation", "ContainerLocation", "GithubLocation", "RemoteFileStorageLocation"]
if TYPE_CHECKING:
    ...
class DataConnection(BaseDataConnection):
    """You need a Data Storage Connection class for Service training metadata (input data).

    :param connection: connection parameters of a specific type
    :type connection: NFSConnection or ConnectionAsset, optional

    :param location: required location parameters of a specific type
    :type location: Union[S3Location, FSLocation, AssetLocation]

    :param data_asset_id: data asset ID, if the DataConnection should point to a data asset
    :type data_asset_id: str, optional

    :param connection_asset_id: connection asset ID, if the DataConnection should point to a connection asset
    :type connection_asset_id: str, optional
    """
    def __init__(self, location: Union[S3Location, FSLocation, AssetLocation, CloudAssetLocation, NFSLocation, DeploymentOutputAssetLocation, ConnectionAssetLocation, DatabaseLocation, ContainerLocation, GithubLocation, RemoteFileStorageLocation,] = ..., connection: Optional[Union[S3Connection, NFSConnection, ConnectionAsset,]] = ..., data_asset_id: str | None = ..., connection_asset_id: str | None = ..., **kwargs: Any) -> None:
        ...
    
    def set_client(self, api_client=..., **kwargs: Any): # -> None:
        """To enable write/read operations with a connection to a service, set an initialized service client in the connection.

        :param api_client: API client to connect to a service
        :type api_client: APIClient

        **Example:**

        .. code-block:: python

            DataConnection.set_client(api_client=api_client)
        """
        ...
    
    @classmethod
    def from_studio(cls, path: str) -> List[DataConnection]:
        """Create DataConnections from the credentials stored (connected) in Watson Studio. Only for COS.

        :param path: path in the COS bucket to the training dataset
        :type path: str

        :return: list with DataConnection objects
        :rtype: list[DataConnection]

        **Example:**

        .. code-block:: python

            data_connections = DataConnection.from_studio(path="iris_dataset.csv")
        """
        ...
    
    def to_dict(self) -> dict:
        """Convert a DataConnection object to a dictionary representation.

        :return: DataConnection dictionary representation
        :rtype: dict
        """
        ...
    
    def __repr__(self): # -> str:
        ...
    
    def __str__(self) -> str:
        ...
    
    @classmethod
    def from_dict(cls, connection_data: dict) -> DataConnection:
        """Create a DataConnection object from a dictionary.

        :param connection_data: dictionary data structure with information about the data connection reference
        :type connection_data: dict

        :return: DataConnection object
        :rtype: DataConnection
        """
        ...
    
    def read(self, with_holdout_split: bool = ..., csv_separator: str = ..., excel_sheet: str | int | None = ..., encoding: str = ..., raw: bool = ..., binary: bool = ..., read_to_file: str | None = ..., number_of_batch_rows: int | None = ..., sampling_type: str | None = ..., sample_size_limit: int | None = ..., sample_rows_limit: int | None = ..., sample_percentage_limit: float | None = ..., **kwargs: Any) -> DataFrame | Tuple[DataFrame, DataFrame] | bytes:
        """Download a dataset that is stored in a remote data storage. Returns batch up to 1 GB.

        :param with_holdout_split: if `True`, data will be split to train and holdout dataset as it was by AutoAI
        :type with_holdout_split: bool, optional

        :param csv_separator: separator/delimiter for the CSV file
        :type csv_separator: str, optional

        :param excel_sheet: excel file sheet name to use, use only when the xlsx file is an input,
            support for the number of the sheet is deprecated
        :type excel_sheet: str, optional

        :param encoding: encoding type of the CSV file
        :type encoding: str, optional

        :param raw: if `False`, simple data is preprocessed (the same as in the backend),
            if `True`, data is not preprocessed
        :type raw: bool, optional

        :param binary: indicates to retrieve data in binary mode, the result will be a python binary type variable
        :type binary: bool, optional

        :param read_to_file: stream read data to a file under the path specified as the value of this parameter,
            use this parameter to prevent keeping data in-memory
        :type read_to_file: str, optional

        :param number_of_batch_rows: number of rows to read in each batch when reading from the flight connection
        :type number_of_batch_rows: int, optional

        :param sampling_type: a sampling strategy on how to read the data
        :type sampling_type: str, optional

        :param sample_size_limit: upper limit for the overall data to be downloaded in bytes, default: 1 GB
        :type sample_size_limit: int, optional

        :param sample_rows_limit: upper limit for the overall data to be downloaded in a number of rows
        :type sample_rows_limit: int, optional

        :param sample_percentage_limit: upper limit for the overall data to be downloaded
            in the percent of all dataset, this parameter is ignored, when `sampling_type` parameter is set
            to `first_n_records`, must be a float number between 0 and 1
        :type sample_percentage_limit: float, optional

        .. note::

            If more than one of: `sample_size_limit`, `sample_rows_limit`, `sample_percentage_limit` are set,
            then downloaded data is limited to the lowest threshold.

        :return: one of the following:

            - pandas.DataFrame that contains dataset from remote data storage : Xy_train
            - Tuple[pandas.DataFrame, pandas.DataFrame, pandas.DataFrame, pandas.DataFrame] : X_train, X_holdout, y_train, y_holdout
            - Tuple[pandas.DataFrame, pandas.DataFrame] : X_test, y_test that contains training data and holdout data from
              remote storage
            - bytes object, auto holdout split from backend (only train data provided)

        **Examples**

        .. code-block:: python

            train_data_connections = optimizer.get_data_connections()

            data = train_data_connections[0].read()  # all train data

            # or

            X_train, X_holdout, y_train, y_holdout = train_data_connections[0].read(
                with_holdout_split=True
            )  # train and holdout data

        Your train and test data:

        .. code-block:: python

            optimizer.fit(
                training_data_reference=[DataConnection],
                training_results_reference=DataConnection,
                test_data_reference=DataConnection,
            )

            test_data_connection = optimizer.get_test_data_connections()
            X_test, y_test = test_data_connection.read()  # only holdout data

            # and

            train_data_connections = optimizer.get_data_connections()
            data = train_connections[0].read()  # only train data
        """
        ...
    
    def write(self, data: Union[str, DataFrame], remote_name: str = ..., **kwargs: Any) -> None:
        """Upload a file to a remote data storage.

        :param data: local path to the dataset or pandas.DataFrame with data
        :type data: str

        :param remote_name: name of dataset to be stored in the remote data storage
        :type remote_name: str
        """
        ...
    
    def download(self, filename: str | Path) -> None:
        """Download a dataset stored in a remote data storage and save to a file.

        :param filename: path to the file where data will be downloaded
        :type filename: str | Path

        **Examples**

        .. code-block:: python

            document_reference = DataConnection(
                connection_asset_id="<connection_id>",
                location=S3Location(bucket="<bucket_name>", path="path/to/file"),
            )
            document_reference.download(filename="results.json")

        """
        ...
    
    def download_folder(self, local_dir: str | Path | None = ...) -> None:
        """Download files from a folder and subfolders stored in a remote data storage and save to a local directory.

        :param local_dir: path to the local directory where data will be downloaded, download to current working directory if not provided
        :type local_dir: str | Path, optional

        **Examples**

        .. code-block:: python

            folder_reference = DataConnection(
                connection_asset_id="<connection_id>",
                location=S3Location(bucket="<bucket_name>", path="path/to/folder"),
            )
            folder_reference.download(local_dir="./data")

        """
        ...
    


class S3Connection(BaseConnection):
    """Connection class to a COS data storage in S3 format.

    :param endpoint_url: URL of the S3 data storage (COS)
    :type endpoint_url: str

    :param access_key_id: access key ID of the S3 connection (COS)
    :type access_key_id: str, optional

    :param secret_access_key: secret access key of the S3 connection (COS)
    :type secret_access_key: str, optional

    :param api_key: API key of the S3 connection (COS)
    :type api_key: str, optional

    :param service_name: service name of the S3 connection (COS)
    :type service_name: str, optional

    :param auth_endpoint: authentication endpoint URL of the S3 connection (COS)
    :type auth_endpoint: str, optional
    """
    def __init__(self, endpoint_url: str, access_key_id: str = ..., secret_access_key: str = ..., api_key: str = ..., service_name: str = ..., auth_endpoint: str = ..., resource_instance_id: str = ..., _internal_use=...) -> None:
        ...
    


class S3Location(BaseLocation):
    """Connection class to a COS data storage in S3 format.

    :param bucket: COS bucket name
    :type bucket: str

    :param path: COS data path in the bucket
    :type path: str

    :param excel_sheet: name of the excel sheet, if the chosen dataset uses an excel file for Batched Deployment scoring
    :type excel_sheet: str, optional

    :param model_location: path to the pipeline model in the COS
    :type model_location: str, optional

    :param training_status: path to the training status JSON in the COS
    :type training_status: str, optional
    """
    def __init__(self, bucket: str, path: str, **kwargs: Any) -> None:
        ...
    
    def get_location(self) -> str:
        ...
    


class ContainerLocation(BaseLocation):
    """Connection class to default COS in user Project/Space."""
    def __init__(self, path: Optional[str] = ..., **kwargs: Any) -> None:
        ...
    
    def to_dict(self) -> dict:
        ...
    
    def get_location(self) -> str:
        ...
    
    def prepend_container_id_to_path(self, container_id: str): # -> None:
        """Prepend project / space ID to path.
        For projects and spaces stored in shared buckets, their ID must be prepended to the path.
        The assignment is skipped if the path already starts with ``container_id``.

        :param container_id: id of project / space
        :type container_id: str
        """
        ...
    


class FSLocation(BaseLocation):
    """Connection class to File Storage in CP4D."""
    def __init__(self, path: Optional[str] = ...) -> None:
        ...
    
    def get_location(self) -> str:
        ...
    


class AssetLocation(BaseLocation):
    def __init__(self, asset_id: str) -> None:
        ...
    
    def to_dict(self) -> dict:
        """Return a json dictionary representing this model."""
        ...
    
    @property
    def wml_client(self): # -> None:
        ...
    
    @wml_client.setter
    def wml_client(self, var): # -> None:
        ...
    
    @property
    def api_client(self): # -> None:
        ...
    
    @api_client.setter
    def api_client(self, var): # -> None:
        ...
    


class ConnectionAssetLocation(BaseLocation):
    """Connection class to a COS data storage.

    :param bucket: COS bucket name
    :type bucket: str

    :param file_name: COS data path in the bucket
    :type file_name: str

    :param model_location: path to the pipeline model in the COS
    :type model_location: str, optional

    :param training_status: path to the training status JSON in COS
    :type training_status: str, optional
    """
    def __init__(self, bucket: str, file_name: str, **kwargs: Any) -> None:
        ...
    
    def to_dict(self) -> dict:
        """Return a json dictionary representing this model."""
        ...
    


class GithubLocation(BaseLocation):
    """Connection class to a Github.

    :param secret_manager_url: url of Secrets Manager service where the Github PAT and url are stored.
    :type secret_manager_url: str

    :param secret_id: ID of the secret with Github PAT and url in the Secrets Manager
    :type secret_id: str

    :param path: path within github repo to the file
    :type path: str
    """
    def __init__(self, secret_manager_url: str, secret_id: str, path: str) -> None:
        ...
    
    def to_dict(self) -> dict:
        """Return a json dictionary representing this model."""
        ...
    


class ConnectionAsset(BaseConnection):
    """Connection class for a Connection Asset.

    :param connection_id: ID of the connection asset
    :type connection_id: str
    """
    def __init__(self, connection_id: str) -> None:
        ...
    


class NFSConnection(BaseConnection):
    """Connection class to file storage in Cloud Pak for Data of NFS format.

    :param asset_id: asset ID of the Cloud Pak for Data project
    :type asset_id: str
    """
    def __init__(self, asset_id: str) -> None:
        ...
    


class NFSLocation(BaseLocation):
    """Location class to file storage in Cloud Pak for Data of NFS format.

    :param path: data path to the Cloud Pak for Data project
    :type path: str
    """
    def __init__(self, path: str) -> None:
        ...
    
    def get_location(self) -> str:
        ...
    


class CloudAssetLocation(AssetLocation):
    """Connection class to data assets as input data references to a batch deployment job on Cloud.

    :param asset_id: asset ID of the file loaded on space on Cloud
    :type asset_id: str
    """
    def __init__(self, asset_id: str) -> None:
        ...
    


class DeploymentOutputAssetLocation(BaseLocation):
    """Connection class to data assets where output of batch deployment will be stored.

    :param name: name of CSV file to be saved as a data asset
    :type name: str
    :param description: description of the data asset
    :type description: str, optional
    """
    def __init__(self, name: str, description: str = ...) -> None:
        ...
    


class DatabaseLocation(BaseLocation):
    """Location class to Database.

    :param schema_name: name of database schema
    :type schema_name: str

    :param table_name: name of database table
    :type table_name: str, optional

    :param catalog_name: name of database catalog, required only for Presto data source
    :type catalog_name: str, optional
    """
    def __init__(self, schema_name: str, table_name: str | None = ..., catalog_name: str | None = ..., **kwargs: Any) -> None:
        ...
    
    def to_dict(self) -> dict:
        """Get a json dictionary representing DatabaseLocation."""
        ...
    


class _AmazonS3Connection(BaseConnection):
    """Connection class to a AmazonS3 data storage in S3 format.
     It's dedicated to work with temporary credentials retrieved from project or space details.

    :param access_key: access key ID (username) for authorizing access to AWS
    :type access_key: str
    :param bucket: name of the bucket that contains the files to access
    :type bucket: str
    :param region: Amazon Web Services (AWS) region. Region name should match the region that Endpoint URL points to.
    :type region: str
    :param secret_key: The password associated with the access key ID for authorizing access to AWS
    :type secret_key: str
    :param session_token: session token associated with access_key and secret_key
    :type session_token: str
    :param shared_credentials: True if the credentials are for shared S3 bucket, False if the credentials are for dedicated S3 bucket. Default is False.
    :type shared_credentials: bool
    """
    def __init__(self, *, access_key: str, bucket: str, region: str, secret_key: str, session_token: str, shared_credentials: bool = ...) -> None:
        ...
    
    def to_dict(self) -> dict:
        """Get a json dictionary representing _AmazonS3Connection."""
        ...
    


class RemoteFileStorageLocation(BaseLocation):
    """Location class to remote file storage in DropBox, Box or Azure Blob Storage.

    :param path: data path to file or folder on remote storage
    :type path: str

    :param container: specific name of the container containing the stored data,
    relevant only to Azure Blob Storage.
    :type container: str, optional

    """
    def __init__(self, path: str, container: str | None = ...) -> None:
        ...
    
    def to_dict(self) -> dict:
        ...
    
    def get_location(self) -> str:
        ...
    


