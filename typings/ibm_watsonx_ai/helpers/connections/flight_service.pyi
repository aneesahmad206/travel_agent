"""
This type stub file was generated by pyright.
"""

import logging
import pandas as pd
import pyarrow as pa
from pathlib import Path
from typing import Generator, Iterable, List, Optional
from ibm_watsonx_ai import APIClient
from pyarrow import flight
from .utils.flight_utils import CallbackSchema, _flight_retry

logger = ...
DEFAULT_PARTITIONS_NUM = ...
DEFAULT_BATCH_SIZE_FLIGHT_COMMAND = ...
DEFAULT_BATCH_SIZE_FLIGHT_COMMAND_BINARY_READ = ...
class BaseFlightConnection:
    """Base class for flight connection."""
    def __init__(self, *, api_client: APIClient | None, _logger: logging.Logger = ...) -> None:
        ...
    


class FlightConnection(BaseFlightConnection):
    """FlightConnection object unify the work for data reading from different types of data sources,
    including databases. It uses a Flight Service and `pyarrow` library to connect and transfer the data.

    .. note::
        All available and supported connection types could be found on:
        https://connectivity-matrix.us-south.cf.test.appdomain.cloud

    :param headers: Service authorization headers to connect with Flight Service
    :type headers: dict

    :param project_id: ID of project
    :type project_id: str

    :param space_id: ID of space
    :type space_id: str

    :param label: Y column name, it is required for subsampling
    :type label: str

    :param sampling_type: a sampling strategy required of choice
    :type sampling_type: str

    :param learning_type: type of the dataset: 'classification', 'multiclass', 'regression', needed for resampling,
        if value is equal to None 'first_n_records' strategy will be used no mather what is specified in
        'sampling_type'
    :type learning_type: str

    :param data_location: data location information passed by user
    :type data_location: dict, optional

    :param enable_subsampling: tells to activate sampling mode for large data
    :type enable_subsampling: bool, optional

    :param callback: required for sending messages
    :type callback: StatusCallback, optional

    :param data_batch_size_limit: upper limit for data in one batch of data that should be downloaded in Bytes,
        default: 1GB
    :type data_batch_size_limit: int, optional

    :param logical_batch_size_limit: upper limit for logical batch when subsampling is turned on (in Bytes),
        default 2GB, the logical batch is the batch that is merged to the subsampled batch (eg. 2GB + 1GB) and then
        subsampling is performed on top of that 3GBs and 1GB batch (subsampled one) is rewritten again
    :type logical_batch_size_limit: int, optional

    :param flight_parameters: pure unchanged flight service parameters that need to be passed to the service
    :type flight_parameters: dict, optional

    :param fallback_to_one_connection: indicates if in case of failure we should switch to the one connection
        and try again, default `True`
    :type fallback_to_one_connection: bool, optional

    :param return_subsampling_stats: indicates whether return batch data stats: dataset size, no. of batches,
        applicable only if subsampling is enabled, default `False`
    :type return_subsampling_stats: bool, optional

    :param total_size_limit: upper limit for overall data that should be downloaded in Bytes, default: 1GB,
        if more than one of: `total_size_limit`, `total_nrows_limit`, `total_percentage_limit` are set,
        then data are limited to the lower threshold, if None, then all data are downloaded in batches
        in `iterable_read` method
    :type total_size_limit: int, optional

    :param total_nrows_limit: upper limit for overall data that should be downloaded in number of rows,
        if more than one of: `total_size_limit`, `total_nrows_limit`, `total_percentage_limit` are set,
        then data are limited to the lower threshold
    :type total_nrows_limit: int, optional

    :param total_percentage_limit: upper limit for overall data that should be downloaded in percent of all dataset,
        must be a float number between 0 and 1, if more than one of: `total_size_limit`, `total_nrows_limit`,
        `total_percentage_limit` are set, then data are limited to the lower threshold
    :type total_percentage_limit: float, optional

    :param apply_literal_eval: when True then ast.literal_eval will be applied to all string columns.
    :type apply_literal_eval: bool, optional

    :param max_retry_time: maximal time for retrying in seconds (the whole retrying process should take less than max_retry_time)
    :type max_retry_time: int, optional

    :param cast_strings: when True then all string columns are cast to float or bool if applicable
    :type cast_strings: bool, optional
    """
    flight_client: flight.FlightClient
    def __init__(self, headers: dict, sampling_type: str, label: str, learning_type: str, params: dict, project_id: Optional[str] = ..., space_id: Optional[str] = ..., asset_id: Optional[str] = ..., connection_id: Optional[str] = ..., data_location: Optional[dict] = ..., enable_subsampling: Optional[bool] = ..., callback: Optional[CallbackSchema] = ..., data_batch_size_limit: Optional[int] = ..., logical_batch_size_limit: Optional[int] = ..., flight_parameters: dict = ..., extra_interaction_properties: dict = ..., fallback_to_one_connection: Optional[bool] = ..., number_of_batch_rows: int = ..., stop_after_first_batch: bool = ..., return_subsampling_stats: bool = ..., total_size_limit=..., total_nrows_limit=..., total_percentage_limit=..., apply_literal_eval: bool = ..., max_retry_time: int = ..., cast_strings: bool = ..., **kwargs) -> None:
        ...
    
    def __enter__(self): # -> Self:
        ...
    
    def close(self): # -> None:
        ...
    
    def __exit__(self, exc_type, exc_value, exc_tb): # -> None:
        ...
    
    @property
    def infer_as_varchar(self): # -> str:
        ...
    
    @infer_as_varchar.setter
    def infer_as_varchar(self, var): # -> None:
        ...
    
    def get_endpoints(self) -> Iterable[List[flight.FlightEndpoint]]:
        """Listing all available Flight Service endpoints (one endpoint corresponds to one batch)"""
        ...
    
    def iterable_read(self) -> Generator:
        """Iterate over batches of data from Flight Service.

        How does it work?

        0. Read, create and yield a subsampled batch.
        1. Start separate threads per Flight partition to read mini batches.
        2. Eg. we have 5 separate threads that read the data (batch by batch and updates the queue).
        3. Start creating the logical batch by create_logical_batch() method. It will consume mini batches
            from the queue and try to create a logical bigger batch.
        4. We have defined a 'batch_queue' list variable that will be storing maximum of 2 logical batches at once.
            For each generated logical batch we perform the following actions:
            a) If 'batch_queue' is empty, append it with first logical batch
            b) Continue logical batch creation...
            c) If 'batch_queue' has one element (batch), append a new batch to it,
                indicates that all Flight data reading threads need to stop downloading the data,
            d) Yield batch from the beginning of the list 'batch_queue'
            e) If we have control back, delete the first batch
            f) Unblock all Flight reading threads
            g) Yield second batch from 'batch_queue'
        5. When we do not have a control flow right now and something other is processing our batch,
            all Flight Threads are working and downloading next data... (we need to ensure that overall queue
            will not overwhelm RAM)
        6. When we get a control flow back, there will be a really fast logical bach creation as
            all of the mini batches needed are already stored in the memory.
        """
        ...
    
    def read(self) -> Generator[pd.DataFrame, None, None]:
        """Fetch the data from Flight Service. Fetching is done in batches.
        There is an upper top limit of data size to be fetched configured to 1 GB.

        :return: fetched data
        :rtype: Generator[pandas.DataFrame, None, None]
        """
        ...
    
    def regression_random_sampling(self, label_column: str) -> None:
        """Start collecting sampled data (random sample for regression problem)"""
        ...
    
    def stratified_subsampling(self, label_column: str) -> None:
        """Start collecting sampled data (stratified sample for classification problem)"""
        ...
    
    def truncate_sampling(self, label_column: str) -> None:
        """Start collecting sampled data (truncate sample for forecasting problem)"""
        ...
    
    def normal_read(self): # -> None:
        """Start collecting all the data when user do not want to subsample.
        This should be limited to the max data size 1GB, see limitation implementation in self._read_data().
        """
        ...
    
    def create_logical_batch(self, timeout: int = ..., _type: str = ...) -> Generator[pd.DataFrame, None, None]:
        """Create a logical batch for sampling, logical batch is larger ~2GB.
        If used with normal read, it will return all of the collected data, max 1GB based on a limitation.
        """
        ...
    
    def read_binary_data(self, read_to_file: str | None = ...) -> Generator | list:
        """Try to read data from flight service using the 'read_raw' parameter. This will allow to fetch binary data.
        Binary read should be used for small data, like json files, zip files etc. not for the big datasets as
        each data batch is joined to the previous one in-memory.
        """
        ...
    
    def write_binary_data(self, file_path: str | Path) -> None:
        """Write data in 16MB binary data blocks. 16MB upper limit is set by the Flight Service.
        The writer will open the source local file and will stream one batch of 16MB to the Flight.
        Only 16MB of data is loaded into the memory at a time.

        :param file_path: path to the source file
        :type file_path: str | Path
        """
        ...
    
    def write_data(self, data: pd.DataFrame): # -> tuple[Any, Any]:
        """Write data from pandas DataFrame. The limit is 16MB dataframe as this is the upper batch size limit.
        Upper layer should fallback to use binary write.
        """
        ...
    
    def get_batch_writer(self, schema: pa.Schema) -> flight.FlightStreamWriter:
        """Prepare FlightStreamWriter and return it."""
        ...
    
    def data_dropna(self, data, label_column):
        ...
    
    @_flight_retry()
    def discovery(self, path: str, details: bool = ...): # -> Any | None:
        ...
    


