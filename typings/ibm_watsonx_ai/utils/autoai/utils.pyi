"""
This type stub file was generated by pyright.
"""

import enum
import io
from pathlib import Path
from contextlib import contextmanager
from typing import Any, Callable, Dict, List, Optional, TYPE_CHECKING, Tuple, Union
from .enums import BatchedClassificationAlgorithms, BatchedRegressionAlgorithms, ClassificationAlgorithms, ForecastingAlgorithms, ImputationStrategy, RegressionAlgorithms, Transformers
from collections import OrderedDict
from io import BufferedIOBase, BytesIO
from ibm_boto3 import resource
from lale.operators import TrainablePipeline
from pandas import DataFrame
from sklearn.pipeline import Pipeline
from ibm_watsonx_ai import APIClient
from ibm_watsonx_ai.helpers import DataConnection

__all__ = ["fetch_pipelines", "load_file_from_file_system", "load_file_from_file_system_nonautoai", "NextRunDetailsGenerator", "prepare_auto_ai_model_to_publish_normal_scenario", "prepare_auto_ai_model_to_publish_notebook_normal_scenario", "prepare_auto_ai_model_to_publish", "remove_file", "ProgressGenerator", "is_ipython", "try_import_lale", "try_load_dataset", "check_dependencies_versions", "try_import_autoai_libs", "try_import_autoai_ts_libs", "try_import_tqdm", "try_import_xlrd", "try_import_openpyxl", "try_import_onnxruntime", "try_import_onnxruntime_extensions", "prepare_onnx_model_to_publish", "try_import_graphviz", "prepare_cos_client", "create_model_download_link", "create_summary", "get_node_and_runtime_index", "download_experiment_details_from_file", "prepare_model_location_path", "download_wml_pipeline_details_from_file", "init_cos_client", "check_graphviz_binaries", "try_import_joblib", "get_sw_spec_and_type_based_on_sklearn", "validate_additional_params_for_optimizer", "is_list_composed_from_enum", "validate_optimizer_enum_values", "all_logging_disabled", "check_if_ts_pipeline_is_winner", "get_values_for_imputation_strategy", "translate_imputation_string_strategy_to_enum", "translate_estimator_string_to_enum", "translate_batched_estimator_string_to_enum", "convert_dataframe_to_fields_values_payload", "get_autoai_run_id_from_experiment_metadata", "run_id_required"]
if TYPE_CHECKING:
    ...
_logger = ...
def create_model_download_link(file_path: str | Path): # -> None:
    """Create download link and shows it in the jupyter notebook.

    :param file_path: path to model file
    :type file_path: str
    """
    ...

def fetch_pipelines(run_params: dict, path: str | Path, api_client: Optional[APIClient] = ..., pipeline_name: str = ..., load_pipelines: bool = ..., store: bool = ..., auto_pipelines_parameters: dict = ..., **kwargs) -> Tuple[Union[None, Dict[str, Pipeline]], bool]:
    """Helper function to download and load computed AutoAI pipelines (sklearn pipelines).

    :param run_params: fetched details of the run/fit
    :type run_params: dict
    :param path: local system path indicates where to store downloaded pipelines
    :type path: str | Path
    :param api_client: API client
    :type api_client: APIClient
    :param pipeline_name: name of the pipeline to download, if not specified, all pipelines are downloaded
    :type pipeline_name: str, optional
    :param load_pipelines: indicator if we load and return downloaded piepelines
    :type load_pipelines: bool, optional
    :param store: indicator to store pipelines in local filesystem
    :type store: bool, optional
    :param auto_pipelines_parameters: auto pipelines parameters
    :type auto_pipelines_parameters: dict, optional

    :return: list of sklearn Pipelines or None if load_pipelines is set to False, with additional information
        about pipeline nodes check
    :rtype: tuple[list[Pipeline], bool]
    """
    ...

def load_file_from_file_system(api_client: APIClient | None = ..., file_path: str | Path | None = ..., stream: bool = ..., **kwargs) -> io.BytesIO:
    """Load file into memory from the file system.

    :param api_client: API client
    :type api_client: APIClient
    :param file_path: path in the file system of the file
    :type file_path: str | Path, optional
    :param stream: indicator to stream data content
    :type stream: bool, optional

    :return: Sklearn Pipeline
    :rtype: io.BytesIO
    """
    ...

async def aload_file_from_file_system(api_client: APIClient, file_path: str | Path, stream: bool = ...) -> io.BytesIO:
    """Load file into memory from the file system asynchronously.

    :param api_client: API client
    :type api_client: APIClient
    :param file_path: path in the file system of the file
    :type file_path: str | Path
    :param stream: indicator to stream data content
    :type stream: bool, optional

    :return: Sklearn Pipeline
    :rtype: io.BytesIO
    """
    ...

def load_file_from_file_system_nonautoai(api_client: APIClient | None = ..., file_path: str | Path | None = ..., stream: bool = ..., **kwargs) -> io.BytesIO:
    """Load file into memory from the file system.

    :param api_client: API client
    :type api_client: APIClient
    :param file_path: path in the file system of the file
    :type file_path: str | Path, optional
    :param stream: indicator to stream data content
    :type stream: bool, optional

    :return: file content
    :rtype: io.BytesIO
    """
    ...

async def aload_file_from_file_system_nonautoai(api_client: APIClient, file_path: str | Path, stream: bool = ...) -> io.BytesIO:
    """Load file into memory from the file system asynchronously.

    :param api_client: API client
    :type api_client: APIClient
    :param file_path: path in the file system of the file
    :type file_path: str | Path
    :param stream: indicator to stream data content
    :type stream: bool, optional

    :return: file content
    :rtype: io.BytesIO
    """
    ...

class NextRunDetailsGenerator:
    """Generator class to produce next list of run details."""
    def __init__(self, api_client: Optional[APIClient] = ..., href: Optional[str] = ..., **kwargs) -> None:
        ...
    
    @property
    def wml_client(self): # -> APIClient | None:
        ...
    
    def __iter__(self): # -> Self:
        ...
    
    def __next__(self): # -> Any:
        ...
    


def preprocess_request_json(request_json: Dict) -> Dict:
    """Removes S3 types from older trainings and replace them with container type."""
    ...

def chose_model_output(model_number: str, is_ml_metrics: bool = ..., is_ts_metrics: bool = ..., run_params: dict = ...) -> str:
    """Chose correct path for particular model number"""
    ...

def prepare_auto_ai_model_to_publish_notebook_normal_scenario(pipeline_model: Union[Pipeline, TrainablePipeline], result_connection, cos_client, run_params: Dict, space_id: str, auto_pipelines_parameters=...) -> tuple[str, dict[str, Any]]:
    """
    ** The method is deprecated since 1.3.31 **

    Prepares autoai model to publish in Watson Studio via COS.
    Option only for auto-gen notebooks with correct result references on COS.

    :param pipeline_model: model object to publish
    :type pipeline_model: Pipeline or TrainablePipeline
    :param result_connection: connection object with COS credentials and all needed locations for jsons
    :type result_connection: DataConnection
    :param cos_client: initialized COS client
    :type cos_client: ibm_boto3.resource
    :param run_params: dictionary with training details
    :type run_params: dict
    :param space_id: UID of space
    :type space_id: str
    :param auto_pipelines_parameters: auto pipelines parameters
    :type auto_pipelines_parameters: dict, optional

    :return: path to the saved model and jsons in COS
    :rtype: tuple[str, dict[str, dict]]
    """
    ...

def prepare_auto_ai_model_to_publish_normal_scenario(pipeline_model: Union[Pipeline, TrainablePipeline], run_params: dict, run_id: str, api_client: Optional[APIClient] = ..., space_id: Optional[str] = ..., result_reference: DataConnection = ..., auto_pipelines_parameters: dict = ..., **kwargs) -> tuple[str, dict[str, Any]]:
    """Helper function to specify `content_location` statement for AutoAI models to store in repository.

    :param pipeline_model: model that will be prepared for an upload
    :type pipeline_model: Pipeline or TrainablePipeline
    :param run_params: fetched details of the run/fit
    :type run_params: dict
    :param run_id: fit/run ID associated with the model
    :type run_id: str
    :param api_client: API Client
    :type api_client: APIClient
    :param space_id: UID of space
    :type space_id: str, optional
    :param result_reference: needed when we have something different than old S3 on a Cloud
    :type result_reference: DataConnection, optional
    :param auto_pipelines_parameters: auto pipelines parameters
    :type auto_pipelines_parameters: dict, optional

    :return:
        if cp4d: dictionary with model schema and artifact name to upload, stored temporally in the user local
            file system
        else: path name to the stored model in COS
    :rtype: dict or str
    """
    ...

async def aprepare_auto_ai_model_to_publish_normal_scenario(pipeline_model: Pipeline | TrainablePipeline, run_params: dict, run_id: str, api_client: APIClient | None = ..., space_id: str | None = ..., result_reference: DataConnection | None = ..., auto_pipelines_parameters: dict | None = ...) -> tuple[str, dict[str, Any]]:
    """Helper function to specify `content_location` statement for AutoAI models to store in repository.

    :param pipeline_model: model that will be prepared for an upload
    :type pipeline_model: Pipeline or TrainablePipeline
    :param run_params: fetched details of the run/fit
    :type run_params: dict
    :param run_id: fit/run ID associated with the model
    :type run_id: str
    :param api_client: API Client
    :type api_client: APIClient
    :param space_id: UID of space
    :type space_id: str, optional
    :param result_reference: needed when we have something different than old S3 on a Cloud
    :type result_reference: DataConnection, optional
    :param auto_pipelines_parameters: auto pipelines parameters
    :type auto_pipelines_parameters: dict, optional

    :return:
        if cp4d: dictionary with model schema and artifact name to upload, stored temporally in the user local
            file system
        else: path name to the stored model in COS
    :rtype: dict or str
    """
    ...

def prepare_auto_ai_model_to_publish(pipeline_model: Union[Pipeline, TrainablePipeline], run_params: dict, run_id: str, api_client: APIClient | None = ..., **kwargs) -> tuple[dict[str, dict], str]:
    """Helper function to download and load computed AutoAI pipelines (sklearn pipelines).

    :param pipeline_model: model that will be prepared for an upload
    :type pipeline_model: Pipeline or TrainablePipeline
    :param run_params: fetched details of the run/fit
    :type run_params: dict
    :param run_id: fit/run ID associated with the model
    :type run_id: str
    :param api_client: API Client
    :type api_client: APIClient

    :return:
        if cp4d: Dictionary with model schema and artifact name to upload, stored temporally in the user local
            file system
        else: path name to the stored model in COS

    :rtype: tuple[dict, str] or str
    """
    ...

async def aprepare_auto_ai_model_to_publish(pipeline_model: Pipeline | TrainablePipeline, run_params: dict, run_id: str, api_client: APIClient | None = ...) -> tuple[dict[str, dict], str]:
    """Helper function to download and load computed AutoAI pipelines (sklearn pipelines).

    :param pipeline_model: model that will be prepared for an upload
    :type pipeline_model: Pipeline or TrainablePipeline
    :param run_params: fetched details of the run/fit
    :type run_params: dict
    :param run_id: fit/run ID associated with the model
    :type run_id: str
    :param api_client: API Client
    :type api_client: APIClient

    :return:
        if cp4d: Dictionary with model schema and artifact name to upload, stored temporally in the user local
            file system
        else: path name to the stored model in COS

    :rtype: tuple[dict, str] or str
    """
    ...

def modify_pipeline_model_json(data_location: str, model_path: str) -> None:
    """Change the location of KB model in pipeline-model.json

    :param data_location: pipeline-model.json data local path
    :type data_location: str

    :param model_path: path to KB model stored in COS
    :type model_path: str
    """
    ...

def init_cos_client(connection: dict) -> resource:
    """Initiate COS client for further usage."""
    ...

def remove_file(filename: str | Path): # -> None:
    """Helper function to clean user local storage from temporary package files."""
    ...

class ProgressGenerator:
    def __init__(self) -> None:
        ...
    
    def get_progress(self, text): # -> int:
        ...
    
    def get_total(self): # -> int:
        ...
    


def is_ipython(): # -> bool:
    """Check if code is running in the notebook."""
    ...

def try_import_lale(): # -> None:
    """Check if lale package is installed in local environment, if not, just download and install it."""
    ...

def try_import_autoai_libs(minimum_version: str = ...): # -> None:
    """Check if autoai_libs package is installed in local environment, if not, just download and install it."""
    ...

def try_import_autoai_ts_libs(): # -> None:
    """Check if autoai_ts_libs package is installed in local environment, if not, just download and install it."""
    ...

def try_import_tqdm(): # -> None:
    """Check if tqdm package is installed in local environment, if not, just download and install it."""
    ...

def try_import_xlrd(): # -> None:
    """Check if xlrd package is installed in local environment, if not, just download and install it."""
    ...

def try_import_graphviz(): # -> None:
    """Check if graphviz package is installed in local environment, if not, just download and install it."""
    ...

def try_import_joblib(): # -> Any:
    """Check if joblib is available from scikit-learn or externally and change 'load' method to inform the user about
    compatibility issues.
    """
    ...

ENCODING_LIST = ...
def try_load_dataset(buffer: Union[BytesIO, BufferedIOBase], sheet_name: str = ..., separator: str = ..., encoding: Optional[str] = ...) -> Union[DataFrame, OrderedDict]:
    """Load data into a pandas DataFrame from BytesIO object.

    :param buffer: buffer with bytes data
    :type buffer: BytesIO or BufferedIOBase

    :param sheet_name: name of the xlsx sheet to read
    :type sheet_name: str, optional

    :param separator: csv separator
    :type separator: str, optional

    :param encoding: data encoding
    :type encoding: str, optional

    :return: loaded dataset
    :rtype: DataFrame or OrderedDict
    """
    ...

def try_import_openpyxl(): # -> None:
    """Check if openpyxl package is installed in local environment, if not, just download and install it."""
    ...

def try_import_onnxruntime(): # -> Any | None:
    """Check if onnxruntime package is installed in local environment, if not return None and print the warning."""
    ...

def try_import_onnxruntime_extensions(): # -> Any | None:
    """Check if onnxruntime package is installed in local environment, if not return None and print the warning."""
    ...

def try_load_tar_gz(buffer: Union[BytesIO, BufferedIOBase], separator: str = ..., encoding: Optional[str] = ...) -> Union[DataFrame, OrderedDict]:
    """Load csv packed into tar.gz into a pandas DataFrame from BytesIO object.

    :param buffer: buffer with bytes data
    :type buffer: BytesIO or BufferedIOBase

    :param separator: csv separator
    :type separator: str, optional

    :param encoding: data encoding
    :type encoding: str, optional

    :return: loaded dataset
    :rtype: DataFrame or OrderedDict
    """
    ...

def check_dependencies_versions(request_json: dict, api_client: Optional[APIClient] = ..., estimator_pkg: Optional[str] = ..., **kwargs) -> bool:
    """Check packages installed versions and inform the user about needed ones.

    :param request_json: dictionary with request from training saved on user COS or CP4D fs
    :type request_json: dict

    :param api_client: internal API client used for sw spec requests
    :type api_client: APIClient

    :param estimator_pkg: name of the estimator package to check with
    :type estimator_pkg: str

    :return: check result
    :rtype: bool
    """
    ...

def prepare_cos_client(training_data_references: List[DataConnection] = ..., training_result_reference: DataConnection = ...) -> Tuple[Union[List[Tuple[DataConnection, resource]]], Union[Tuple[DataConnection, resource], None],]:
    """Create COS clients for training data and results.

    :param training_data_references: references to training data
    :type training_data_references: list[DataConnection], optional

    :param training_result_reference: reference to training result
     :type training_result_reference: DataConnection, optional

    :return: list of COS clients for training data, client for results
    :rtype: tuple[list[tuple[DataConnection, resource]], tuple[DataConnection, resource]]
    """
    ...

def create_summary(details: dict, scoring: str, sort_by_holdout_score: bool = ...) -> DataFrame:
    """Create summary in a form of a pandas.DataFrame of computed pipelines (should be used in remote and local scenario
    with COS).

    :param details: dictionary with all training data
    :type details: type

    :param scoring: scoring method
    :type scoring: str

    :param sort_by_holdout_score: indicates if we want to sort pipelines by holdout metric (or by training one),
        by default use holdout metric
    :type sort_by_holdout_score: bool, optional

    :return: pipelines summary
    :rtype: pandas.DataFrame
    """
    ...

def get_node_and_runtime_index(node_name: str, optimizer_config: dict) -> Tuple[int, int]:
    """Find node index from node name in experiment parameters."""
    ...

def download_experiment_details_from_file(result_client_and_connection: Tuple[DataConnection, resource]) -> dict:
    """Try to download training details from user COS."""
    ...

def download_wml_pipeline_details_from_file(result_client_and_connection: Tuple[DataConnection, resource]) -> dict:
    """Try to download pipeline details from user COS."""
    ...

def prepare_model_location_path(model_path: str) -> str:
    """To be able to get best pipeline after computation we need to change model_location string to global_output."""
    ...

def check_graphviz_binaries(f): # -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]:
    ...

def get_sw_spec_and_type_based_on_sklearn(client: APIClient, spec: str) -> Tuple[str, str]:
    """Based on user environment and pipeline sw spec, check sklearn version and find appropriate sw spec.

    :param client: API Client
    :type client: APIClient

    :param spec: software specification name
    :type spec: str

    :return: model_type and sw_spec
    :rtype: tuple[str, str]
    """
    ...

def validate_additional_params_for_optimizer(params): # -> None:
    ...

def download_request_json(run_params: dict, model_name: str, api_client=..., results_reference: DataConnection = ..., auto_pipelines_parameters=..., **kwargs) -> dict:
    ...

async def adownload_request_json(run_params: dict, model_name: str, api_client: APIClient, results_reference: DataConnection, auto_pipelines_parameters: dict | None = ...) -> dict:
    ...

def is_list_composed_from_enum(sequence: List[Union[str, enum.Enum]], enum_class: Union[object, enum.EnumMeta]) -> None:
    """Check if all the elements of a given sequence are values of a given enum class.

    :param sequence: sequence of elements
    :type sequence: list[str or enum.Enum]
    :param enum_class: class for which validation will be performed, it can be a class inheriting from enum.Enum or
        class which only contains attributes
    :type enum_class: object or enum.EnumMeta

    :raises InvalidSequenceValue: risen if element is not from enum class values
    """
    ...

def validate_optimizer_enum_values(prediction_type: str, daub_include_only_estimators: List[Union[ClassificationAlgorithms, RegressionAlgorithms, ForecastingAlgorithms]], include_only_estimators: List[Union[ClassificationAlgorithms, RegressionAlgorithms, ForecastingAlgorithms]], include_batched_ensemble_estimators: List[Union[BatchedClassificationAlgorithms, BatchedRegressionAlgorithms]], cognito_transform_names: List[Transformers], imputation_strategies: List[ImputationStrategy], scoring: str, t_shirt_size: str, is_cpd=...) -> None:
    """Validate if passed optimizer variables takes values from defined enums.

    :param prediction_type: type of the prediction
    :type prediction_type: str

    :param daub_include_only_estimators: list of estimators
    :type daub_include_only_estimators: list

    :param include_only_estimators: list of estimators
    :type include_only_estimators: list

    :param include_batched_ensemble_estimators: list of batched ensemble estimators
    :type include_batched_ensemble_estimators: list

    :param cognito_transform_names: list of transformers
    :type cognito_transform_names: list

    :param imputation_strategies: list of imputation strategies
    :type imputation_strategies: list

    :param scoring: type of the metric to optimize with
    :type scoring: str

    :param t_shirt_size: size of the remote AutoAI POD instance
    :type t_shirt_size: str

    :param is_cpd: `True` if run on CP4D environment, CP4D estimators will be used to check
    :type is_cpd: bool

    :raises InvalidSequenceValue: risen if element is not from enum class values
    """
    ...

@contextmanager
def all_logging_disabled(highest_level=...): # -> Generator[None, Any, None]:
    """A context manager that will prevent any logging messages
    triggered during the body from being processed.

    :param highest_level: the maximum logging level in use
        This would only need to be changed if a custom level greater than CRITICAL
        is defined
    :type highest_level: int
    """
    ...

def check_if_ts_pipeline_is_winner(details: dict, model_name: str) -> None:
    """Check if ts pipeline is the winner one. It should be used before model store in the repo."""
    ...

def get_values_for_imputation_strategy(strategy, prediction_type, imputer_fill_value=...): # -> dict[str, Any]:
    ...

def translate_imputation_string_strategy_to_enum(strategy, prediction_type): # -> list[list[Any] | ImputationStrategy] | ImputationStrategy:
    ...

def translate_estimator_string_to_enum(estimator): # -> str:
    ...

def translate_batched_estimator_string_to_enum(estimator): # -> str:
    ...

def convert_dataframe_to_fields_values_payload(df: DataFrame, return_values_only: bool = ..., onnx_mode: bool = ...) -> dict[str, Any] | list:
    ...

def get_autoai_run_id_from_experiment_metadata(experiment_metadata: dict) -> str:
    ...

def run_id_required(func: Callable) -> Callable:
    ...

def prepare_onnx_model_to_publish(model: str, run_params: dict, client: APIClient) -> tuple[str, dict]:
    """Read ONNX request & update model metadata"""
    ...

