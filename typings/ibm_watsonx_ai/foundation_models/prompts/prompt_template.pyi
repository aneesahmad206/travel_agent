"""
This type stub file was generated by pyright.
"""

import langchain
import pandas
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Literal, TYPE_CHECKING, TypeAlias, overload
from langchain.prompts import ChatPromptTemplate, PromptTemplate as LcPromptTemplate
from ibm_watsonx_ai import APIClient, Credentials
from ibm_watsonx_ai.foundation_models.prompts.base_prompt_template import BasePromptTemplate
from ibm_watsonx_ai.foundation_models.prompts.chat_prompt import ChatPrompt
from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, PromptTemplateFormats
from ibm_watsonx_ai.wml_resource import WMLResource

if TYPE_CHECKING:
    ...
ListType: TypeAlias = list
@dataclass
class PromptTemplateLock:
    """Storage for lock object."""
    locked: bool
    lock_type: str | None = ...
    locked_by: str | None = ...


class FreeformPromptTemplate(BasePromptTemplate):
    """Storage for Freeform prompt template asset parameters.

    :param prompt_id: ID of the prompt template, defaults to None.
    :type prompt_id: str, attribute setting not allowed

    :param created_at: time that the prompt was created (UTC), defaults to None.
    :type created_at: str, attribute setting not allowed

    :param lock: locked state of the asset, defaults to None.
    :type lock: PromptTemplateLock | None, attribute setting not allowed

    :param is_template: True if the prompt is a template, False otherwise; defaults to None.
    :type is_template: bool | None, attribute setting not allowed

    :param name: name of the prompt template, defaults to None.
    :type name: str, optional

    :param model_id: ID of the foundation model, defaults to None.
    :type model_id: ModelTypes | str | None, optional

    :param model_params: parameters of the model, defaults to None.
    :type model_params: dict, optional

    :param template_version: semantic version for tracking in IBM AI Factsheets, defaults to None.
    :type template_version: str, optional

    :param task_ids: list of task IDs, defaults to None.
    :type task_ids: list[str] | None, optional

    :param description: description of the prompt template asset, defaults to None.
    :type description: str, optional

    :param input_text: input text for the prompt, defaults to None.
    :type input_text: str, optional

    :param input_variables: input variables can be present in field `input_text`
                            and are identified by braces ('{' and '}'), defaults to None.
    :type input_variables: (list | dict[str, dict[str, str]]), optional

    :param validate_template: if True, the prompt template is validated for the presence of input variables, defaults to True.
    :type validate_template: bool, optional

    :raises ValidationError: raised when the set of input_variables is not consistent with the input variables present in the template.
                             Raised only when `validate_template` is set to True.

    **Examples**

    Example of an invalid Freeform prompt template:

    .. code-block:: python

        prompt_template = FreeformPromptTemplate(
            name="My freeform prompt",
            model_id="ibm/granite-13b-chat-v2",
            input_text="What are the most famous monuments in ?",
            input_variables=["country"],
        )

        # Traceback (most recent call last):
        #    ...
        # ValidationError: Invalid prompt template; check for mismatched or missing input variables. Missing input variable: {'country'}

    Example of a valid Freeform prompt template:

    .. code-block:: python

        prompt_template = FreeformPromptTemplate(
            name="My freeform prompt",
            model_id="ibm/granite-13b-chat-v2"
            input_text='What are the most famous monuments in {country}?',
            input_variables=['country']
        )

    """
    _input_mode = ...
    def __init__(self, name: str | None = ..., model_id: ModelTypes | str | None = ..., model_params: dict | None = ..., template_version: str | None = ..., task_ids: list[str] | None = ..., description: str | None = ..., input_text: str | None = ..., input_variables: list | dict[str, dict[str, str]] | None = ..., validate_template: bool = ...) -> None:
        ...
    


class PromptTemplate(BasePromptTemplate):
    """Parameter storage for a structured prompt template.

    :param prompt_id: ID of the prompt template, defaults to None.
    :type prompt_id: str, attribute setting not allowed

    :param created_at: time that the prompt was created (UTC), defaults to None.
    :type created_at: str, attribute setting not allowed

    :param lock: locked state of the asset, defaults to None.
    :type lock: PromptTemplateLock | None, attribute setting not allowed

    :param is_template: True if the prompt is a template, False otherwise; defaults to None.
    :type is_template: bool | None, attribute setting not allowed

    :param name: name of the prompt template, defaults to None.
    :type name: str, optional

    :param model_id: ID of the Foundation model, defaults to None.
    :type model_id: ModelTypes | str | None, optional

    :param model_params: parameters of the model, defaults to None.
    :type model_params: dict, optional

    :param template_version: semantic version for tracking in IBM AI Factsheets, defaults to None.
    :type template_version: str, optional

    :param task_ids: List of task IDs, defaults to None.
    :type task_ids: list[str] | None, optional

    :param description: description of the prompt template asset, defaults to None.
    :type description: str, optional

    :param input_text: input text for the prompt, defaults to None.
    :type input_text: str, optional

    :param input_variables: Input variables can be present in fields: `instruction`,
                            `input_prefix`, `output_prefix`, `input_text`, `examples`
                            and are identified by braces ('{' and '}'), defaults to None.
    :type input_variables: (list | dict[str, dict[str, str]]), optional

    :param instruction: instruction for the model, defaults to None.
    :type instruction: str, optional

    :param input_prefix: prefix string placed before the input text, defaults to None.
    :type input_prefix: str, optional

    :param output_prefix: prefix placed before the model response, defaults to None.
    :type output_prefix: str, optional

    :param examples: examples that might help the model adjust the response; [[input1, output1], ...], defaults to None.
    :type examples: list[list[str]], optional

    :param validate_template: if True, the prompt template is validated for the presence of input variables, defaults to True.
    :type validate_template: bool, optional

    :raises ValidationError: raised when the set of input_variables is not consistent with the input variables present in the template.
                             Raised only when `validate_template` is set to True.

    **Examples**

    Example of an invalid prompt template:

    .. code-block:: python

        prompt_template = PromptTemplate(
            name="My structured prompt",
            model_id="ibm/granite-13b-chat-v2"
            input_text='What are the most famous monuments in ?',
            input_variables=['country']
        )

        # Traceback (most recent call last):
        #     ...
        # ValidationError: Invalid prompt template; check for mismatched or missing input variables. Missing input variable: {'country'}

    Example of a valid prompt template:

    .. code-block:: python

        prompt_template = PromptTemplate(
            name="My structured prompt",
            model_id="ibm/granite-13b-chat-v2"
            input_text='What are the most famous monuments in {country}?',
            input_variables=['country']
        )

    """
    _input_mode = ...
    def __init__(self, name: str | None = ..., model_id: ModelTypes | str | None = ..., model_params: dict | None = ..., template_version: str | None = ..., task_ids: list[str] | None = ..., description: str | None = ..., input_text: str | None = ..., input_variables: list | dict[str, dict[str, str]] | None = ..., instruction: str | None = ..., input_prefix: str | None = ..., output_prefix: str | None = ..., examples: list[list[str]] | None = ..., validate_template: bool = ..., **kwargs: Any) -> None:
        ...
    


class DetachedPromptTemplate(BasePromptTemplate):
    """Storage for detached prompt template parameters.

    :param prompt_id: ID of the prompt template, defaults to None.
    :type prompt_id: str, attribute setting not allowed

    :param created_at: time that the prompt was created (UTC), defaults to None.
    :type created_at: str, attribute setting not allowed

    :param lock: locked state of the asset, defaults to None.
    :type lock: PromptTemplateLock | None, attribute setting not allowed

    :param is_template: True if the prompt is a template, False otherwise; defaults to None.
    :type is_template: bool | None, attribute setting not allowed

    :param name: name of the prompt template, defaults to None.
    :type name: str, optional

    :param model_id: ID of the foundation model, defaults to None.
    :type model_id: ModelTypes | str | None, optional

    :param model_params: parameters of the model, defaults to None.
    :type model_params: dict, optional

    :param template_version: semantic version for tracking in IBM AI Factsheets, defaults to None.
    :type template_version: str, optional

    :param task_ids: list of task IDs, defaults to None.
    :type task_ids: list[str] | None, optional

    :param description: description of the prompt template asset, defaults to None.
    :type description: str, optional

    :param input_text: input text for the prompt, defaults to None.
    :type input_text: str, optional

    :param input_variables: input variables can be present in field: `input_text`
                            and are identified by braces ('{' and '}'), defaults to None.
    :type input_variables: (list | dict[str, dict[str, str]]), optional

    :param detached_prompt_id: ID of the external prompt, defaults to None
    :type detached_prompt_id: str | None, optional

    :param detached_model_id: ID of the external model, defaults to None
    :type detached_model_id: str | None, optional

    :param detached_model_provider: external model provider, defaults to None
    :type detached_model_provider: str | None, optional

    :param detached_prompt_url: URL for the external prompt, defaults to None
    :type detached_prompt_url: str | None, optional

    :param detached_prompt_additional_information: additional information of the external prompt, defaults to None
    :type detached_prompt_additional_information: list[dict[str, Any]] | None, optional

    :param detached_model_name: name of the external model, defaults to None
    :type detached_model_name: str | None, optional

    :param detached_model_url: URL for the external model, defaults to None
    :type detached_model_url: str | None, optional

    :param validate_template: if True, the prompt template is validated for the presence of input variables, defaults to True
    :type validate_template: bool, optional

    :param instruction: instruction for the model, defaults to None
    :type instruction: str, optional

    :param input_prefix: prefix string placed before the input text, defaults to None
    :type input_prefix: str, optional

    :param output_prefix: prefix placed before the model response, defaults to None
    :type output_prefix: str, optional

    :param examples: examples that might help the model adjust the response; [[input1, output1], ...], defaults to None
    :type examples: list[list[str]], optional

    :raises ValidationError: raised when the set of input_variables is not consistent with the input variables present in the template.
                             Raised only when `validate_template` is set to True.

    **Examples**

    Example of an invalid detached prompt template:

    .. code-block:: python

        prompt_template = DetachedPromptTemplate(
            name="My detached prompt",
            model_id="<some model>",
            input_text="What are the most famous monuments in ?",
            input_variables=["country"],
            detached_prompt_id="<prompt id>",
            detached_model_id="<model id>",
            detached_model_provider="<provider>",
            detached_prompt_url="<url>",
            detached_prompt_additional_information=[{"key": "value"}],
            detached_model_name="<model name>",
            detached_model_url="<model url>",
        )

        # Traceback (most recent call last):
        #     ...
        # ValidationError: Invalid prompt template; check for mismatched or missing input variables. Missing input variable: {'country'}

    Example of a valid detached prompt template:

    .. code-block:: python

        prompt_template = DetachedPromptTemplate(
            name="My detached prompt",
            model_id="<some model>",
            input_text="What are the most famous monuments in {country}?",
            input_variables=["country"],
            detached_prompt_id="<prompt id>",
            detached_model_id="<model id>",
            detached_model_provider="<provider>",
            detached_prompt_url="<url>",
            detached_prompt_additional_information=[{"key": "value"}],
            detached_model_name="<model name>",
            detached_model_url="<model url>",
        )

    """
    _input_mode = ...
    def __init__(self, name: str | None = ..., model_id: ModelTypes | str | None = ..., model_params: dict | None = ..., template_version: str | None = ..., task_ids: list[str] | None = ..., description: str | None = ..., input_text: str | None = ..., input_variables: list | dict[str, dict[str, str]] | None = ..., detached_prompt_id: str | None = ..., detached_model_id: str | None = ..., detached_model_provider: str | None = ..., detached_prompt_url: str | None = ..., detached_prompt_additional_information: list[dict[str, Any]] | None = ..., detached_model_name: str | None = ..., detached_model_url: str | None = ..., validate_template: bool = ..., instruction: str | None = ..., input_prefix: str | None = ..., output_prefix: str | None = ..., examples: list[list[str]] | None = ...) -> None:
        ...
    


class PromptTemplateManager(WMLResource):
    """Instantiate the prompt template manager.

    :param credentials: credentials for the watsonx.ai instance
    :type credentials: Credentials or dict, optional

    :param project_id: ID of the project
    :type project_id: str, optional

    :param space_id: ID of the space
    :type space_id: str, optional

    :param verify: You can pass one of the following as verify:
        * the path to a CA_BUNDLE file
        * the path of directory with certificates of trusted CAs
        * `True` - default path to truststore will be taken
        * `False` - no verification will be made
    :type verify: bool | str | Path, optional

    .. note::
        One of these parameters is required: ['project_id ', 'space_id']

    **Example:**

    .. code-block:: python

        from ibm_watsonx_ai import Credentials

        from ibm_watsonx_ai.foundation_models.prompts import (
            PromptTemplate,
            PromptTemplateManager,
        )

        prompt_mgr = PromptTemplateManager(
            credentials=Credentials(
                api_key=IAM_API_KEY, url="https://us-south.ml.cloud.ibm.com"
            ),
            project_id="*****",
        )

        prompt_template = PromptTemplate(
            name="My prompt",
            model_id="meta-llama/llama-3-3-70b-instruct",
            input_prefix="Human:",
            output_prefix="Assistant:",
            input_text="What is {object} and how does it work?",
            input_variables=["object"],
            examples=[
                [
                    "What is the Stock Market?",
                    "A stock market is a place where investors buy and sell shares of publicly traded companies.",
                ]
            ],
        )

        stored_prompt_template = prompt_mgr.store_prompt(prompt_template)
        print(stored_prompt_template.prompt_id)  # id of prompt template asset

    .. note::
        Here's an example of how you can pass variables to your deployed prompt template:

        .. code-block:: python

            from ibm_watsonx_ai.metanames import GenTextParamsMetaNames

            meta_props = {
                client.deployments.ConfigurationMetaNames.NAME: "SAMPLE DEPLOYMENT PROMPT TEMPLATE",
                client.deployments.ConfigurationMetaNames.ONLINE: {},
                client.deployments.ConfigurationMetaNames.BASE_MODEL_ID: "meta-llama/llama-3-3-70b-instruct",
            }

            deployment_details = client.deployments.create(
                stored_prompt_template.prompt_id, meta_props
            )

            client.deployments.generate_text(
                deployment_id=deployment_details["metadata"]["id"],
                params={
                    GenTextParamsMetaNames.PROMPT_VARIABLES: {"object": "brain"}
                },
            )

    """
    def __init__(self, credentials: Credentials | dict | None = ..., *, project_id: str | None = ..., space_id: str | None = ..., verify: str | Path | bool | None = ..., api_client: APIClient | None = ...) -> None:
        ...
    
    @overload
    def load_prompt(self, prompt_id: str, astype: Literal[PromptTemplateFormats.STRING, "string"], *, prompt_variables: dict[str, str] | None = ...) -> str:
        ...
    
    @overload
    def load_prompt(self, prompt_id: str, astype: Literal[PromptTemplateFormats.LANGCHAIN, "langchain"], *, prompt_variables: dict[str, str] | None = ...) -> LcPromptTemplate:
        ...
    
    @overload
    def load_prompt(self, prompt_id: str, astype: Literal[PromptTemplateFormats.PROMPTTEMPLATE, "prompt"] = ..., *, prompt_variables: dict[str, str] | None = ...) -> (FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt):
        ...
    
    def load_prompt(self, prompt_id: str, astype: PromptTemplateFormats | str = ..., *, prompt_variables: dict[str, str] | None = ...) -> (FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt | str | LcPromptTemplate | ChatPromptTemplate):
        """Retrieve a prompt template asset.

        :param prompt_id: ID of the processed prompt template
        :type prompt_id: str

        :param astype: type of return object
        :type astype: PromptTemplateFormats

        :param prompt_variables: dictionary of input variables and values that will replace the input variables
        :type prompt_variables: dict[str, str]

        :return: prompt template asset
        :rtype: FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt | str | langchain.prompts.PromptTemplate

        **Example:**

        .. code-block:: python

            loaded_prompt_template = prompt_mgr.load_prompt(prompt_id)
            loaded_prompt_template_lc = prompt_mgr.load_prompt(
                prompt_id, PromptTemplateFormats.LANGCHAIN
            )
            loaded_prompt_template_string = prompt_mgr.load_prompt(
                prompt_id, PromptTemplateFormats.STRING
            )
        """
        ...
    
    def list(self, *, limit: int | None = ...) -> pandas.DataFrame:
        """List all available prompt templates in the DataFrame format.

        :param limit: limit number of fetched records, defaults to None.
        :type limit: int, optional

        :return: DataFrame of fundamental properties of available prompts.
        :rtype: pandas.core.frame.DataFrame

        **Example:**

        .. code-block:: python

            prompt_mgr.list(
                limit=5
            )  # list of 5 recent created prompt template assets

        .. hint::
            Additionally you can sort available prompt templates by "LAST MODIFIED" field.

            .. code-block:: python

                df_prompts = prompt_mgr.list()
                df_prompts.sort_values("LAST MODIFIED", ascending=False)

        """
        ...
    
    def store_prompt(self, prompt_template: (FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt | langchain.prompts.PromptTemplate)) -> FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt:
        """Store a new prompt template.

        :param prompt_template: PromptTemplate to be stored.
        :type prompt_template: (FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt | langchain.prompts.PromptTemplate)

        :return: PromptTemplate object that is initialized with values provided in the server response object.
        :rtype: FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt
        """
        ...
    
    def delete_prompt(self, prompt_id: str, *, force: bool = ...) -> str:
        """Remove a prompt template from a project or space.

        :param prompt_id: ID of the prompt template to be deleted
        :type prompt_id: str

        :param force: if True, then the prompt template is unlocked and then deleted, defaults to False.
        :type force: bool

        :return: status 'SUCCESS' if the prompt template is successfully deleted
        :rtype: str

        **Example:**

        .. code-block:: python

            prompt_mgr.delete_prompt(prompt_id)  # delete if asset is unlocked
        """
        ...
    
    def update_prompt(self, prompt_id: str, prompt_template: (FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt)) -> dict:
        """Update prompt template data.

        :param prompt_id: ID of the prompt template to be updated
        :type prompt_id: str

        :param prompt_template: prompt template with new data
        :type prompt_template: FreeformPromptTemplate | PromptTemplate | DetachedPromptTemplate | ChatPrompt

        :return: metadata of the updated deployment
        :rtype: dict

        **Example:**

        .. code-block:: python

            updated_prompt_template = PromptTemplate(name="New name")
            prompt_mgr.update_prompt(
                prompt_id, prompt_template
            )  # {'name': 'New name'} in metadata

        """
        ...
    
    def get_lock(self, prompt_id: str) -> dict:
        """Get the current locked state of a prompt template.

        :param prompt_id: ID of the prompt template
        :type prompt_id: str

        :return: information about the locked state of a prompt template asset
        :rtype: dict

        **Example:**

        .. code-block:: python

            print(prompt_mgr.get_lock(prompt_id))
        """
        ...
    
    def lock(self, prompt_id: str, force: bool = ...) -> dict:
        """Lock a prompt template if it is unlocked and you have permission to lock it.

        :param prompt_id: ID of the prompt template
        :type prompt_id: str

        :param force: if True, lock is forcefully overwritten
        :type force: bool

        :return: locked prompt template
        :rtype: dict

        **Example:**

        .. code-block:: python

            prompt_mgr.lock(prompt_id)

        """
        ...
    
    def unlock(self, prompt_id: str) -> dict:
        """Unlock a prompt template if it is locked and you have permission to unlock it.

        :param prompt_id: ID of the prompt template
        :type prompt_id: str

        :return: unlocked prompt template
        :rtype: dict

        **Example:**

        .. code-block:: python

            prompt_mgr.unlock(prompt_id)
        """
        ...
    
    def add_chat_items(self, prompt_id: str, chat_items: ListType[dict]) -> str:
        """Add a new chat items to a prompt.

        :param prompt_id: ID of the prompt template
        :type prompt_id: str

        :param chat_items: Chat items to be added to prompt
        :type chat_items: list[dict]

        :return: status ("SUCCESS" if succeeded)
        :rtype: str

        **Example:**

        .. code-block:: python

            prompt_mgr.add_chat_items(
                prompt_id="<PROMPT_ID>",
                chat_items=[
                    {
                        "type": "<CHAT ITEM TYPE>",
                        "content": "<CHAT ITEM CONTENT>",
                        "status": "<CHAT ITEM STATUS>",
                        "timestamp": <TIMESTAMP_AS_INT>,
                    },
                    {
                        "type": "<CHAT ITEM TYPE>",
                        "content": "<CHAT ITEM CONTENT>",
                        "status": "<CHAT ITEM STATUS>",
                        "timestamp": <TIMESTAMP_AS_INT>,
                    }
                ]
            )

        """
        ...
    


