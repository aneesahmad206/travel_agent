"""
This type stub file was generated by pyright.
"""

from abc import ABC, abstractmethod
from typing import AsyncGenerator, Generator, Literal, TYPE_CHECKING
from ibm_watsonx_ai.foundation_models.schema import TextChatParameters, TextGenParameters
from ibm_watsonx_ai.wml_resource import WMLResource
from ibm_watsonx_ai import APIClient

if TYPE_CHECKING:
    ...
__all__ = ["BaseModelInference"]
_RETRY_STATUS_CODES = ...
LIMIT_RATE_HEADER = ...
class BaseModelInference(WMLResource, ABC):
    """Base interface class for the model interface."""
    DEFAULT_CONCURRENCY_LIMIT = ...
    def __init__(self, name: str, client: APIClient, persistent_connection: bool = ..., max_retries: int | None = ..., delay_time: float | None = ..., retry_status_codes: list[int] | None = ..., validate: bool = ...) -> None:
        ...
    
    @abstractmethod
    def get_details(self) -> dict:
        """Get model interface's details

        :return: details of model or deployment
        :rtype: dict
        """
        ...
    
    @abstractmethod
    def chat(self, messages: list[dict], params: dict | TextChatParameters | None = ..., tools: list | None = ..., tool_choice: dict | None = ..., tool_choice_option: Literal["none", "auto"] | None = ..., context: str | None = ...) -> dict:
        """
        Given a messages as input, and parameters the selected inference
        will generate a chat response.
        """
        ...
    
    @abstractmethod
    def chat_stream(self, messages: list[dict], params: dict | TextChatParameters | None = ..., tools: list | None = ..., tool_choice: dict | None = ..., tool_choice_option: Literal["none", "auto"] | None = ..., context: str | None = ...) -> Generator:
        """
        Given a messages as input, and parameters the selected inference
        will generate a chat as generator.
        """
        ...
    
    @abstractmethod
    async def achat(self, messages: list[dict], params: dict | TextChatParameters | None = ..., tools: list | None = ..., tool_choice: dict | None = ..., tool_choice_option: Literal["none", "auto"] | None = ..., context: str | None = ...) -> dict:
        ...
    
    @abstractmethod
    async def achat_stream(self, messages: list[dict], params: dict | TextChatParameters | None = ..., tools: list | None = ..., tool_choice: dict | None = ..., tool_choice_option: Literal["none", "auto"] | None = ..., context: str | None = ...) -> AsyncGenerator:
        """
        Given a messages as input, and parameters the selected inference
        will generate a chat as a async generator.
        """
        ...
    
    @abstractmethod
    def generate(self, prompt: str | list | None = ..., params: dict | TextGenParameters | None = ..., guardrails: bool = ..., guardrails_hap_params: dict | None = ..., guardrails_pii_params: dict | None = ..., concurrency_limit: int = ..., async_mode: bool = ..., validate_prompt_variables: bool = ..., guardrails_granite_guardian_params: dict | None = ...) -> dict | list[dict] | Generator:
        """
        Given a text prompt as input, and parameters the selected inference
        will generate a completion text as generated_text response.
        """
        ...
    
    @abstractmethod
    async def agenerate_stream(self, prompt: str | None = ..., params: dict | TextGenParameters | None = ..., guardrails: bool = ..., guardrails_hap_params: dict | None = ..., guardrails_pii_params: dict | None = ..., validate_prompt_variables: bool = ..., guardrails_granite_guardian_params: dict | None = ...) -> AsyncGenerator:
        """
        Given a text prompt as input, and parameters the selected inference
        will return async generator with response.
        """
        ...
    
    @abstractmethod
    def generate_text_stream(self, prompt: str | None = ..., params: dict | TextGenParameters | None = ..., raw_response: bool = ..., guardrails: bool = ..., guardrails_hap_params: dict | None = ..., guardrails_pii_params: dict | None = ..., validate_prompt_variables: bool = ..., guardrails_granite_guardian_params: dict | None = ...) -> Generator:
        """
        Given a text prompt as input, and parameters the selected inference
        will generate a completion text as generator.
        """
        ...
    
    @abstractmethod
    def tokenize(self, prompt: str, return_tokens: bool = ...) -> dict:
        ...
    
    @abstractmethod
    def get_identifying_params(self) -> dict:
        """Represent Model Inference's setup in dictionary"""
        ...
    


