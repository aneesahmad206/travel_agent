"""
This type stub file was generated by pyright.
"""

from typing import AsyncGenerator, Generator, Literal, TYPE_CHECKING, overload
from ibm_watsonx_ai.foundation_models.schema import TextChatParameters, TextGenParameters
from .base_model_inference import BaseModelInference
from ibm_watsonx_ai import APIClient

__all__ = ["FMModelInference"]
if TYPE_CHECKING:
    ...
class FMModelInference(BaseModelInference):
    """Base abstract class for the model interface."""
    def __init__(self, *, model_id: str, api_client: APIClient, params: dict | TextChatParameters | TextGenParameters | None = ..., validate: bool = ..., persistent_connection: bool = ..., max_retries: int | None = ..., delay_time: float | None = ..., retry_status_codes: list[int] | None = ...) -> None:
        ...
    
    def get_details(self) -> dict:
        """Get model's details

        :return: details of model or deployment
        :rtype: dict
        """
        ...
    
    def chat(self, messages: list[dict], params: dict | TextChatParameters | None = ..., tools: list | None = ..., tool_choice: dict | None = ..., tool_choice_option: Literal["none", "auto"] | None = ..., context: str | None = ...) -> dict:
        ...
    
    def chat_stream(self, messages: list[dict], params: dict | TextChatParameters | None = ..., tools: list | None = ..., tool_choice: dict | None = ..., tool_choice_option: Literal["none", "auto"] | None = ..., context: str | None = ...) -> Generator:
        ...
    
    async def achat(self, messages: list[dict], params: dict | TextChatParameters | None = ..., tools: list | None = ..., tool_choice: dict | None = ..., tool_choice_option: Literal["none", "auto"] | None = ..., context: str | None = ...) -> dict:
        ...
    
    async def achat_stream(self, messages: list[dict], params: dict | TextChatParameters | None = ..., tools: list | None = ..., tool_choice: dict | None = ..., tool_choice_option: Literal["none", "auto"] | None = ..., context: str | None = ...) -> AsyncGenerator:
        ...
    
    @overload
    def generate(self, prompt: str | list | None = ..., params: dict | TextGenParameters | None = ..., guardrails: bool = ..., guardrails_hap_params: dict | None = ..., guardrails_pii_params: dict | None = ..., concurrency_limit: int = ..., async_mode: Literal[False] = ..., validate_prompt_variables: bool = ..., guardrails_granite_guardian_params: dict | None = ...) -> dict | list[dict]:
        ...
    
    @overload
    def generate(self, prompt: str | list | None, params: dict | TextGenParameters | None, guardrails: bool, guardrails_hap_params: dict | None, guardrails_pii_params: dict | None, concurrency_limit: int, async_mode: Literal[True], validate_prompt_variables: bool, guardrails_granite_guardian_params: dict | None) -> Generator:
        ...
    
    @overload
    def generate(self, prompt: str | list | None = ..., params: dict | TextGenParameters | None = ..., guardrails: bool = ..., guardrails_hap_params: dict | None = ..., guardrails_pii_params: dict | None = ..., concurrency_limit: int = ..., async_mode: bool = ..., validate_prompt_variables: bool = ..., guardrails_granite_guardian_params: dict | None = ...) -> dict | list[dict] | Generator:
        ...
    
    def generate(self, prompt: str | list | None = ..., params: dict | TextGenParameters | None = ..., guardrails: bool = ..., guardrails_hap_params: dict | None = ..., guardrails_pii_params: dict | None = ..., concurrency_limit: int = ..., async_mode: bool = ..., validate_prompt_variables: bool = ..., guardrails_granite_guardian_params: dict | None = ...) -> dict | list[dict] | Generator:
        """
        Given a text prompt as input, and parameters the selected inference
        will generate a completion text as generated_text response.
        """
        ...
    
    async def agenerate_stream(self, prompt: str | None = ..., params: dict | TextGenParameters | None = ..., guardrails: bool = ..., guardrails_hap_params: dict | None = ..., guardrails_pii_params: dict | None = ..., validate_prompt_variables: bool = ..., guardrails_granite_guardian_params: dict | None = ...) -> AsyncGenerator:
        """
        Given a text prompt as input, and parameters the selected inference
        will generate a completion text as async generator response.
        """
        ...
    
    def generate_text_stream(self, prompt: str | None = ..., params: dict | TextGenParameters | None = ..., raw_response: bool = ..., guardrails: bool = ..., guardrails_hap_params: dict | None = ..., guardrails_pii_params: dict | None = ..., validate_prompt_variables: bool = ..., guardrails_granite_guardian_params: dict | None = ...) -> Generator:
        """
        Given a text prompt as input, and parameters the selected inference
        will generate a completion text as generator.
        """
        ...
    
    def tokenize(self, prompt: str, return_tokens: bool = ...) -> dict:
        """
        Given a text prompt as input, and return_tokens parameter will return tokenized input text.
        """
        ...
    
    def get_identifying_params(self) -> dict:
        """Represent Model Inference's setup in dictionary"""
        ...
    


