"""
This type stub file was generated by pyright.
"""

from typing import Any, Literal, Sequence
from langchain_core.documents import Document
from .base_chunker import BaseChunker

__all__ = ["LangChainChunker"]
class LangChainChunker(BaseChunker[Document]):
    """
    Wrapper for LangChain TextSplitter.

    :param method: describes the type of TextSplitter as the main instance performing the chunking, defaults to "recursive"
    :type method: Literal["recursive", "character", "token"], optional

    :param chunk_size: maximum size of a single chunk that is returned, defaults to 4000
    :type chunk_size: int, optional

    :param chunk_overlap: overlap in characters between chunks, defaults to 200
    :type chunk_overlap: int, optional

    :param encoding_name: encoding used in the TokenTextSplitter, defaults to "gpt2"
    :type encoding_name: str, optional

    :param model_name: model used in the TokenTextSplitter
    :type model_name: str, optional

    .. code-block:: python

        from ibm_watsonx_ai.foundation_models.extensions.rag.chunker import (
            LangChainChunker,
        )

        text_splitter = LangChainChunker(
            method="recursive", chunk_size=1000, chunk_overlap=200
        )

        chunks_ids = []

        for i, document in enumerate(data_loader):
            chunks = text_splitter.split_documents([document])
            chunks_ids.append(
                vector_store.add_documents(chunks, batch_size=300)
            )
    """
    supported_methods = ...
    def __init__(self, method: Literal["recursive", "character", "token"] = ..., chunk_size: int = ..., chunk_overlap: int = ..., encoding_name: str = ..., model_name: str | None = ..., **kwargs: Any) -> None:
        ...
    
    def __eq__(self, other: object) -> bool:
        ...
    
    def to_dict(self) -> dict[str, Any]:
        """
        Return dictionary that can be used to recreate an instance of the LangChainChunker.
        """
        ...
    
    @classmethod
    def from_dict(cls, d: dict[str, Any]) -> LangChainChunker:
        """Create an instance from the dictionary."""
        ...
    
    def split_documents(self, documents: Sequence[Document]) -> list[Document]:
        """
        Split series of documents into smaller chunks based on the provided
        chunker settings. Each chunk has metadata that includes the document_id,
        sequence_number, and start_index.

        :param documents: sequence of elements that contain context in a text format
        :type documents: Sequence[langchain_core.documents.Document]

        :return: list of documents split into smaller ones, having less content
        :rtype: list[langchain_core.documents.Document]
        """
        ...
    


