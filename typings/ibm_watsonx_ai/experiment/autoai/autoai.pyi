"""
This type stub file was generated by pyright.
"""

from pathlib import Path
from typing import Any, List, TYPE_CHECKING, Union
from ibm_watsonx_ai.utils.autoai.enums import BatchedClassificationAlgorithms, BatchedRegressionAlgorithms, ClassificationAlgorithms, DataConnectionTypes, ForecastingAlgorithms, ForecastingPipelineTypes, ImputationStrategy, Metrics, PipelineTypes, PredictionType, RAGMetrics, RegressionAlgorithms, SamplingTypes, TShirtSize, TimeseriesAnomalyPredictionAlgorithms, TimeseriesAnomalyPredictionPipelineTypes, Transformers
from ibm_watsonx_ai.workspace import WorkSpace
from ..base_experiment.base_experiment import BaseExperiment
from .optimizers import LocalAutoPipelines, RAGOptimizer, RemoteAutoPipelines
from .runs import AutoPipelinesRuns, LocalAutoPipelinesRuns
from ...credentials import Credentials
from ...foundation_models.schema import AutoAIRAGCustomModelConfig, AutoAIRAGDeploymentConfig, AutoAIRAGGenerationConfig, AutoAIRAGModelConfig, AutoAIRAGRetrievalConfig

__all__ = ["AutoAI"]
if TYPE_CHECKING:
    ...
class AutoAI(BaseExperiment):
    """AutoAI class for automizing pipeline model optimization.

    :param credentials: credentials to instance
    :type credentials: dict

    :param project_id: ID of the Watson Studio project
    :type project_id: str, optional

    :param space_id: ID of the Watson Studio Space
    :type space_id: str, optional

    :param verify: You can pass one of the following as verify:

        * the path to a CA_BUNDLE file
        * the path of directory with certificates of trusted CAs
        * `True` - takes the default path to the truststore
        * `False` - makes no verification
    :type verify: bool | str | Path, optional

    **Example:**

    .. code-block:: python

        from ibm_watsonx_ai.experiment import AutoAI

        experiment = AutoAI(
            credentials={
                "apikey": IAM_API_KEY,
                "iam_apikey_description": "...",
                "iam_apikey_name": "...",
                "iam_role_crn": "...",
                "iam_serviceid_crn": "...",
                "instance_id": "...",
                "url": "https://us-south.ml.cloud.ibm.com",
            },
            project_id="...",
            space_id="...",
        )
    """
    ClassificationAlgorithms = ...
    RegressionAlgorithms = ...
    ForecastingAlgorithms = ...
    TShirtSize = TShirtSize
    PredictionType = PredictionType
    Metrics = Metrics
    Transformers = Transformers
    DataConnectionTypes = DataConnectionTypes
    PipelineTypes = PipelineTypes
    SamplingTypes = SamplingTypes
    RAGMetrics = RAGMetrics
    def __init__(self, credentials: Credentials | dict | WorkSpace | None = ..., project_id: str | None = ..., space_id: str | None = ..., verify: bool | str | Path | None = ..., **kwargs) -> None:
        ...
    
    def runs(self, *, filter: str) -> Union[AutoPipelinesRuns, LocalAutoPipelinesRuns]:
        """Get the historical runs with a Pipeline name filter (for remote scenario).
        Get the historical runs with an experiment name filter (for local scenario).

        :param filter: Pipeline name to filter the historical runs or experiment name to filter
            the local historical runs
        :type filter: str

        :return: object that manages the list of runs
        :rtype: AutoPipelinesRuns or LocalAutoPipelinesRuns

        **Example:**

        .. code-block:: python

            from ibm_watsonx_ai.experiment import AutoAI

            experiment = AutoAI(...)
            experiment.runs(filter="Test").list()
        """
        ...
    
    def optimizer(self, name: str, *, prediction_type: PredictionType, prediction_column: str = ..., prediction_columns: List[str] = ..., timestamp_column_name: str = ..., scoring: Metrics = ..., desc: str = ..., test_size: float = ..., holdout_size: float | int | None = ..., max_number_of_estimators: int = ..., train_sample_rows_test_size: float = ..., include_only_estimators: List[Union[ClassificationAlgorithms, RegressionAlgorithms, ForecastingAlgorithms, TimeseriesAnomalyPredictionAlgorithms,]] = ..., daub_include_only_estimators: List[Union[ClassificationAlgorithms, RegressionAlgorithms]] = ..., include_batched_ensemble_estimators: List[Union[BatchedClassificationAlgorithms, BatchedRegressionAlgorithms]] = ..., backtest_num: int = ..., lookback_window: int = ..., forecast_window: int = ..., backtest_gap_length: int = ..., feature_columns: List[str] = ..., pipeline_types: List[Union[ForecastingPipelineTypes, TimeseriesAnomalyPredictionPipelineTypes]] = ..., supporting_features_at_forecast: bool = ..., cognito_transform_names: List[Transformers] = ..., csv_separator: Union[List[str], str] = ..., excel_sheet: Union[str, int] = ..., encoding: str = ..., positive_label: str = ..., drop_duplicates: bool = ..., outliers_columns: list = ..., text_processing: bool = ..., word2vec_feature_number: int = ..., daub_give_priority_to_runtime: float = ..., fairness_info: dict = ..., sampling_type: SamplingTypes = ..., sample_size_limit: int = ..., sample_rows_limit: int = ..., sample_percentage_limit: float = ..., n_parallel_data_connections: int = ..., number_of_batch_rows: int = ..., categorical_imputation_strategy: ImputationStrategy = ..., numerical_imputation_strategy: ImputationStrategy = ..., numerical_imputation_value: float = ..., imputation_threshold: float = ..., retrain_on_holdout: bool = ..., categorical_columns: list = ..., numerical_columns: list = ..., test_data_csv_separator: Union[List[str], str] = ..., test_data_excel_sheet: str = ..., test_data_encoding: str = ..., confidence_level: float = ..., incremental_learning: bool = ..., early_stop_enabled: bool = ..., early_stop_window_size: int = ..., time_ordered_data: bool = ..., feature_selector_mode: str = ..., **kwargs) -> Union[RemoteAutoPipelines, LocalAutoPipelines]:
        r"""
        Initialize an AutoAI optimizer.

        :param name: name of the AutoPipelines
        :type name: str

        :param prediction_type: the type of prediction
        :type prediction_type: PredictionType

        :param prediction_column: name of the target/label column, required for `multiclass`, `binary`, and `regression`
            prediction types
        :type prediction_column: str, optional

        :param prediction_columns: names of the target/label columns, required for `forecasting` prediction type
        :type prediction_columns: list[str], optional

        :param timestamp_column_name: name of the timestamp column for time series forecasting
        :type timestamp_column_name: str, optional

        :param scoring: type of the metric to optimize with, not used for forecasting
        :type scoring: Metrics, optional

        :param desc: description
        :type desc: str, optional

        :param test_size: deprecated, use `holdout_size` instead

        :param holdout_size: percentage of the entire dataset to leave as a holdout, for AutoAI Forecasting it can be a number of rows of data
        :type holdout_size: float | int, optional

        :param max_number_of_estimators: maximum number (top-K ranked by DAUB model selection)
            of the selected algorithm, or estimator types, for example `LGBMClassifierEstimator`,
            `XGBoostClassifierEstimator`, or `LogisticRegressionEstimator` to use in the pipeline composition,
            the default is `None` which means that the true default value is determined by
            the internal different algorithms which uses only the algorithm type that is ranked the highest by the model selection
        :type max_number_of_estimators: int, optional

        :param train_sample_rows_test_size: percentage of training data sampling
        :type train_sample_rows_test_size: float, optional

        :param daub_include_only_estimators: deprecated, use `include_only_estimators` instead

        :param include_batched_ensemble_estimators: list of batched ensemble estimators to include
            in the computation process, see: AutoAI.BatchedClassificationAlgorithms, AutoAI.BatchedRegressionAlgorithms
        :type include_batched_ensemble_estimators:
            list[BatchedClassificationAlgorithms or BatchedRegressionAlgorithms], optional

        :param include_only_estimators: list of estimators to include in the computation process, see:
            AutoAI.ClassificationAlgorithms, AutoAI.RegressionAlgorithms or AutoAI.ForecastingAlgorithms
        :type include_only_estimators: List[ClassificationAlgorithms or RegressionAlgorithms or ForecastingAlgorithms]], optional

        :param backtest_num: number of backtests used for forecasting prediction type, default value: 4,
            value from range [0, 20]
        :type backtest_num: int, optional

        :param lookback_window: length of lookback window used for forecasting prediction type,
            default value: 10, if set to -1 lookback window will be auto-detected
        :type lookback_window: int, optional

        :param forecast_window: length of forecast window used for forecasting prediction type, default value: 1,
            value from range [1, 60]
        :type forecast_window: int, optional

        :param backtest_gap_length: gap between backtests used for forecasting prediction type,
            default value: 0, value from range [0, data length / 4]
        :type backtest_gap_length: int, optional

        :param feature_columns: list of feature columns used for the forecasting prediction type,
            might contain target column and/or supporting feature columns, list of columns to be detected whether there are anomalies for timeseries anomaly prediction type
        :type feature_columns: list[str], optional

        :param pipeline_types: list of pipeline types to be used for forecasting or timeseries anomaly prediction type
        :type pipeline_types: list[ForecastingPipelineTypes or TimeseriesAnomalyPredictionPipelineTypes], optional

        :param supporting_features_at_forecast: enables the use of future supporting feature values during the forecast
        :type supporting_features_at_forecast: bool, optional

        :param cognito_transform_names: list of transformers to include in the feature enginnering computation process,
            see: AutoAI.Transformers
        :type cognito_transform_names: list[Transformers], optional

        :param csv_separator: the separator or list of separators for separating columns in a CSV file,
            not used if the file_name is not a CSV file, default is ','
        :type csv_separator: list[str] or str, optional

        :param excel_sheet: name of the excel sheet to use, only applicable when the xlsx file is an input,
            support for number of the sheet is deprecated, by default first sheet is used
        :type excel_sheet: str, optional

        :param encoding: encoding type for the CSV training file
        :type encoding: str, optional

        :param positive_label: the positive class to report when binary classification, when multiclass or regression,
            this is ignored
        :type positive_label: str, optional

        :param t_shirt_size: size of the remote AutoAI POD instance (computing resources),
            only applicable to a remote scenario, see: AutoAI.TShirtSize
        :type t_shirt_size: TShirtSize, optional

        :param drop_duplicates: if `True`, duplicated rows in data are removed before further processing
        :type drop_duplicates: bool, optional

        :param outliers_columns: replace outliers with NaN using the IQR method for specified columns, by default,
            turned ON for regression learning_type and target column, to turn OFF pass an empty list of columns
        :type outliers_columns: list, optional

        :param text_processing: if `True` text processing will be enabled, applicable only on Cloud
        :type text_processing: bool, optional

        :param word2vec_feature_number: number of features to be generated from the text column,
            applied only if `text_processing` is `True`, if `None` the default value will be taken
        :type word2vec_feature_number: int, optional

        :param daub_give_priority_to_runtime: the importance of run time over score for pipelines ranking,
            can take values between 0 and 5, if set to 0.0 only score is used,
            if set to 1 equally score and runtime are used, if set to value higher than 1
            the runtime gets higher importance over score
        :type daub_give_priority_to_runtime: float, optional

        :param fairness_info: dictionary that specifies the metadata needed for measuring fairness,
            it contains three key values: `favorable_labels`, `unfavorable_labels`, and `protected_attributes`,
            the `favorable_labels` attribute indicates a positive outcome when the class column contains one of the values from list,
            the `unfavorable_labels` is opposite to the `favorable_labels`
            and is obligatory for the regression learning type, `protected_attributes` is a list of features that partition
            the population into groups whose outcome should have parity, if `protected_attributes` is an empty list
            then automatic detection of protected attributes is run,
            if `fairness_info` is passed then the fairness metric is calculated
        :type fairness_info: fairness_info

        :param n_parallel_data_connections: number of maximum parallel connection to data source,
            supported only for IBM Cloud Pak® for Data 4.0.1 and later
        :type n_parallel_data_connections: int, optional

        :param categorical_imputation_strategy: missing values imputation strategy for categorical columns

            Possible values (only non-forecasting scenario):

            - ImputationStrategy.MEAN
            - ImputationStrategy.MEDIAN
            - ImputationStrategy.MOST_FREQUENT (default)

        :type categorical_imputation_strategy: ImputationStrategy, optional


        :param numerical_imputation_strategy: missing values imputation strategy for numerical columns

            Possible values (non-forecasting scenario):

            - ImputationStrategy.MEAN
            - ImputationStrategy.MEDIAN (default)
            - ImputationStrategy.MOST_FREQUENT

            Possible values (forecasting scenario):

            - ImputationStrategy.MEAN
            - ImputationStrategy.MEDIAN
            - ImputationStrategy.BEST_OF_DEFAULT_IMPUTERS (default)
            - ImputationStrategy.VALUE
            - ImputationStrategy.FLATTEN_ITERATIVE
            - ImputationStrategy.LINEAR
            - ImputationStrategy.CUBIC
            - ImputationStrategy.PREVIOUS
            - ImputationStrategy.NEXT
            - ImputationStrategy.NO_IMPUTATION

        :param numerical_imputation_value: value for filling missing values if numerical_imputation_strategy
            is set to ImputationStrategy.VALUE, for forecasting only
        :type numerical_imputation_value: float, optional

        :param imputation_threshold: maximum threshold of missing values imputation, for forecasting only
        :type imputation_threshold: float, optional

        :param retrain_on_holdout: if `True`, final pipelines are trained also on holdout data
        :type retrain_on_holdout: bool, optional

        :param categorical_columns: list of columns names to be treated as categorical
        :type categorical_columns: list, optional

        :param numerical_columns: list of columns names to be treated as numerical
        :type numerical_columns: list, optional

        :param sampling_type: type of sampling data for training, one of SamplingTypes enum values,
            default is SamplingTypes.FIRST_N_RECORDS, supported only for IBM Cloud Pak® for Data 4.0.1 and later
        :type sampling_type: str, optional

        :param sample_size_limit: size of the sample upper bound (in bytes). The default value is 1 GB,
            supported only for IBM Cloud Pak® for Data 4.5 and later
        :type sample_size_limit: int, optional

        :param sample_rows_limit: size of the sample upper bound (in rows),
            supported only for IBM Cloud Pak® for Data 4.6 and later
        :type sample_rows_limit: int, optional

        :param sample_percentage_limit: size of the sample upper bound (as fraction of dataset size),
            supported only for IBM Cloud Pak® for Data 4.6 and later
        :type sample_percentage_limit: float, optional

        :param number_of_batch_rows: number of rows to read in each batch when reading from the flight connection
        :type number_of_batch_rows: int, optional

        :param test_data_csv_separator: the separator or list of separators for separating
            columns in a CSV user-defined holdout/test file, not used if the file_name is not a CSV file,
            default is ','
        :type test_data_csv_separator: list[str] or str, optional

        :param test_data_excel_sheet: name of the excel sheet to use for user-defined holdout/test data,
            use only when the xlsx file is a test, dataset file, by default first sheet is used
        :type test_data_excel_sheet: str or int, optional

        :param test_data_encoding: encoding type for the CSV user-defined holdout/test file
        :type test_data_encoding: str, optional

        :param confidence_level: when the pipeline "PointwiseBoundedHoltWinters" or "PointwiseBoundedBATS" is used,
            the prediction interval is calculated at a given confidence_level to decide if a data record
            is an anomaly or not, optional for timeseries anomaly prediction
        :type confidence_level: float, optional

        :param incremental_learning: triggers incremental learning process for supported pipelines
        :type incremental_learning: bool, optional

        :param early_stop_enabled: enables early stop for incremental learning process
        :type early_stop_enabled: bool, optional

        :param early_stop_window_size: the number of iterations without score improvements before the training stops
        :type early_stop_window_size: int, optional

        :param time_ordered_data: defines your preference about time-based analysis, if `True`, the analysis
            considers the data as time-ordered and time-based, supported only for regression
        :type time_ordered_data: bool, optional

        :param feature_selector_mode: defines if feature selector should be triggered ["on", "off", "auto"]
                the "auto" mode analyzes the impact of removing insignificant features, if there is a drop in accuracy,
                the PCA is applied to insignificant features, principal components that describe variance in 30% or higher
                are selected in place of insignificant features and the model is evaluated again, if there is still a drop
                in accuracy, all features are used
                the "on" mode removes all insignificant features (0.0. importance), the feature selector is applied during
                cognito phase (applicable to pipelines with feature engineering stage)
        :type feature_selector_mode: str, optional

        :param \**kwargs: Additional keyword arguments for AutoAI configuration.

        :Keyword Arguments:
            * **datetime_processing_flag** (*bool*) - When enabled, detects date column and adds new columns for different types of date/time format aggregations.

        :return: RemoteAutoPipelines or LocalAutoPipelines, depends on how you initialize the AutoAI object
        :rtype: RemoteAutoPipelines or LocalAutoPipelines

        **Examples**

        .. code-block:: python

            from ibm_watsonx_ai.experiment import AutoAI

            experiment = AutoAI(...)

            fairness_info = {
                "protected_attributes": [
                    {
                        "feature": "Sex",
                        "reference_group": ["male"],
                        "monitored_group": ["female"],
                    },
                    {
                        "feature": "Age",
                        "reference_group": [[50, 60]],
                        "monitored_group": [[18, 49]],
                    },
                ],
                "favorable_labels": ["No Risk"],
                "unfavorable_labels": ["Risk"],
            }

            optimizer = experiment.optimizer(
                name="name of the optimizer.",
                prediction_type=AutoAI.PredictionType.BINARY,
                prediction_column="y",
                scoring=AutoAI.Metrics.ROC_AUC_SCORE,
                desc="Some description.",
                holdout_size=0.1,
                max_number_of_estimators=1,
                fairness_info=fairness_info,
                cognito_transform_names=[
                    AutoAI.Transformers.SUM,
                    AutoAI.Transformers.MAX,
                ],
                train_sample_rows_test_size=1,
                include_only_estimators=[
                    AutoAI.ClassificationAlgorithms.LGBM,
                    AutoAI.ClassificationAlgorithms.XGB,
                ],
                t_shirt_size=AutoAI.TShirtSize.L,
            )

            optimizer = experiment.optimizer(
                name="name of the optimizer.",
                prediction_type=AutoAI.PredictionType.MULTICLASS,
                prediction_column="y",
                scoring=AutoAI.Metrics.ROC_AUC_SCORE,
                desc="Some description.",
            )
        """
        ...
    
    def rag_optimizer(self, name: str, *, description: str | None = ..., chunking: list[dict] | None = ..., embedding_models: list[str] | None = ..., retrieval_methods: list[str] | None = ..., foundation_models: (list[str | dict | AutoAIRAGModelConfig | AutoAIRAGCustomModelConfig] | None) = ..., max_number_of_rag_patterns: int | None = ..., optimization_metrics: list[str] | None = ..., generation: dict[str, Any] | AutoAIRAGGenerationConfig | None = ..., retrieval: list[dict[str, Any] | AutoAIRAGRetrievalConfig] | None = ..., deployment: dict[str, Any] | AutoAIRAGDeploymentConfig | None = ..., **kwargs: Any) -> RAGOptimizer:
        """Initialize an AutoAi RAG optimizer.

        :param name: name for the RAGOptimizer
        :type name: str

        :param description: description for the RAGOptimizer
        :type description: str, optional

        :param chunking: chunking configuration to be used.
        :type chunking: list[dict], optional

        :param embedding_models: The embedding models to try.
        :type embedding_models: list[str], optional

        :param retrieval_methods: Retrieval methods to be used.
        :type retrieval_methods: list[str], optional

        :param foundation_models: List of foundation models to try. Custom foundation models and model config are also supported for Cloud and CPD >= 5.2.
        :type foundation_models: list[str | dict | AutoAIRAGModelConfig | AutoAIRAGCustomModelConfig], optional

        :param max_number_of_rag_patterns: The maximum number of RAG patterns to create.
        :type max_number_of_rag_patterns: int, optional

        :param optimization_metrics: The metric name(s) to be used for optimization.
        :type optimization_metrics: list[str], optional

        :param generation: Properties describing the generation step.
        :type generation: dict[str, Any] | AutoAIRAGGenerationConfig, optional

        :param retrieval: Retrieval settings to be used.
        :type retrieval: list[dict[str, Any] | AutoAIRAGRetrievalConfig], optional

        :param deployment: Best pattern deployment related properties.
        :type deployment: dict[str, Any] | AutoAIRAGDeploymentConfig, optional

        .. deprecated:: IBM Cloud Pak® for Data 5.2
           The parameter ``retrieval_methods`` is deprecated and will be removed in a future version.
           Use ``retrieval`` instead.

        .. deprecated:: IBM Cloud Pak® for Data 5.2
           The parameter ``foundation_models`` is deprecated and will be removed in a future version.
           Use ``generation.foundation_models`` instead.

        :return: AutoAI RAG optimizer
        :rtype: RAGOptimizer

        **Examples**

        - RAG Optimizer configuration with defaults

        .. code-block:: python

            from ibm_watsonx_ai.experiment import AutoAI

            experiment = AutoAI(...)

            optimizer = experiment.rag_optimizer(
                name="RAG - AutoAI", description="Sample description"
            )


        - RAG Optimizer configuration with example values of parameters

        .. code-block:: python

            from ibm_watsonx_ai.experiment import AutoAI

            experiment = AutoAI(...)

            optimizer = experiment.rag_optimizer(
                name="RAG - AutoAI",
                description="Sample description",
                max_number_of_rag_patterns=5,
                chunking=[
                    {
                        "method": "recursive",
                        "chunk_size": 1024,
                        "chunk_overlap": 64,
                    },
                    {"method": "semantic", "chunk_size": 1024},
                ],
                embedding_models=["ibm/slate-125m-english-rtrvr-v2"],
                retrieval=[
                    {"method": "window", "number_of_chunks": 3, "window_size": 2},
                    {"method": "simple", "number_of_chunks": 5},
                ],
                generation={
                    "language": {"auto-detect": True},
                    "foundation_models": [
                        {"model_id": "meta-llama/llama-3-3-70b-instruct"}
                    ],
                },
                optimization_metrics=["answer_correctness"],
                deployment={
                    "inference_service": {
                        "space_id": ...,
                        "auto_deploy": True,
                    },
                    "indexing_service": {
                        "space_id": ...,
                        "auto_deploy": True,
                    },
                },
            )

        - RAG Optimizer quick deployment
        """
        ...
    


